{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مقدمة\n",
    "التعلم المعزز هو أحد فروع تعلم الآلة الذي يعتمد على فكرة التدريب من خلال التجربة والخطأ.\n",
    "في هذا النوع من التعلم، يتفاعل الوكيل (Agent) مع بيئة معينة (Environment) \n",
    "ويحاول اتخاذ قرارات من أجل تحقيق الهدف المطلوب.\n",
    "الهدف هو تحسين سياسة (Policy) \n",
    "الوكيل بحيث يحصل على أكبر مكافأة ممكنة (Reward) على المدى الطويل.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "\n",
    "أمثلة على البيئات:\n",
    "\n",
    "Atari: ألعاب فيديو مثل Breakout,Pong.\n",
    "CartPole: محاولة إبقاء عصا متوازنة على عربة متحركة.\n",
    "MountainCar: دفع سيارة صغيرة للوصول إلى قمة جبل.\n",
    "هذه البيئات متاحة في مكتبات مثل OpenAI Gym، \n",
    "وهي مكتبة مفتوحة المصدر تحتوي على العديد من البيئات المجهزة لاختبار وتطوير خوارزميات التعلم المعزز.\n",
    "\n",
    "تحديد الوكيل (Agent): الوكيل هو النموذج الذي سيتم تدريبه لاتخاذ القرارات.\n",
    " الوكيل يتعلم من خلال التفاعل مع البيئة. كلما تعلم أكثر من خلال التجربة، كلما تحسنت قراراته.\n",
    "\n",
    "اختيار خوارزمية التعلم (Learning Algorithm):\n",
    " هناك عدة خوارزميات يمكن استخدامها في التعلم المعزز، كل منها يتعامل مع التفاعل بين الوكيل والبيئة بشكل مختلف.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "أنواع الخوارزميات:\n",
    "\n",
    "Q-Learning:\n",
    "\n",
    "تعتمد على بناء جدول يسمى Q-Table يحتوي على القيم الممكنة لكل حالة وإجراء.\n",
    "الوكيل يقوم بتحديث هذا الجدول بناءً على المكافآت التي يحصل عليها.\n",
    "Deep Q-Networks (DQN):\n",
    "\n",
    "مشابهة لـ Q-Learning ولكن بدلاً من استخدام جدول، تستخدم شبكة عصبية للتنبؤ بالقيم.\n",
    "مناسبة عندما يكون عدد الحالات كبير جداً.\n",
    "Policy Gradient Methods:\n",
    "\n",
    "تعتمد على تحسين السياسة مباشرةً بدلاً من تحسين القيم.\n",
    "تستخدم عادة عندما يكون هناك عدد كبير من الإجراءات الممكنة.\n",
    "Proximal Policy Optimization (PPO):\n",
    "\n",
    "نوع محسّن من Policy Gradient Methods.\n",
    "يعمل على استقرار التدريب وتقليل التغيرات الكبيرة في السياسة.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "مكونات الخوارزميات وبدائلها\n",
    "الوكيل (Agent):\n",
    "\n",
    "الوكيل هو النموذج الذي يتعلم من خلال التجربة.\n",
    "يتفاعل مع البيئة باتخاذ إجراءات والحصول على مكافآت.\n",
    "البيئة (Environment):\n",
    "\n",
    "هي المكان الذي يتفاعل فيه الوكيل.\n",
    "تحتوي على حالات وأفعال ومكافآت.\n",
    "السياسة (Policy):\n",
    "\n",
    "هي الاستراتيجية التي يتبعها الوكيل لاتخاذ القرارات.\n",
    "يمكن أن تكون السياسة عشوائية في البداية، ولكنها تتحسن مع الوقت.\n",
    "الوظيفة القيمية (Value Function):\n",
    "\n",
    "تُستخدم لتقدير المكافأة المستقبلية المحتملة من كل حالة.\n",
    "تُستخدم في بعض الخوارزميات مثل Q-Learning.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# تشغيل البيئات في بايثون باستخدام مكتبة Gym\n",
    "# لمعرفة البيئات التي تحتويها المكتبة\n",
    "from gym import envs\n",
    "envs.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# إنشاء البيئة\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# تعيد البيئة إلى حالتها الأولية مع توفير حالة جديدة للوكيل للبدء منها.\n",
    "# مهمة لبدء حلقة جديدة من التدريب بعد انتهاء الحلقة السابقة\n",
    "state = env.reset()\n",
    "\n",
    "# حلقة محاكاة\n",
    "for _ in range(1000):\n",
    "    env.render()  # هذه الدالة تعرض البيئة بشكل بصري.\n",
    "\n",
    "    # هذه الدالة تقوم بتوليد إجراء عشوائي صالح من مجموعة الإجراءات الممكنة في البيئة.\n",
    "    # تستخدم عادة في المراحل الأولية من التدريب أو لتجريب بيئة جديدة.    \n",
    "    # مثل التحرك يمين-يسار-فوق-لاسفل\n",
    "    # يمكن برمجة الإجراءات بشكل يدوي أو عن طريق استخدام نموذج تعلم معزز مثل DQN أو A2C \n",
    "    # بدلاً من استخدام إجراءات عشوائية.\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # هذه الدالة هي الأساس في عملية التفاعل بين الوكيل (Agent) والبيئة.\n",
    "    # عند استدعاء env.step(action)، يقوم الوكيل بتنفيذ إجراء معين (action) في البيئة. بعد تنفيذ الإجراء،\n",
    "    # تقوم البيئة بتحديث حالتها وتعيد القيم التالية:\n",
    "    # next_state: الحالة الجديدة التي تصل إليها البيئة بعد تنفيذ الإجراء.\n",
    "    # reward: المكافأة التي يحصل عليها الوكيل نتيجة لتنفيذ الإجراء. الهدف من الوكيل هو تعظيم هذه المكافآت.\n",
    "    # done: قيمة منطقية (Boolean) تشير إلى ما إذا كانت اللعبة أو الحلقة قد انتهت. إذا كانت done = True،\n",
    "    # يجب إعادة تعيين البيئة باستخدام env.reset().\n",
    "    # info: معلومات إضافية قد تقدمها البيئة\n",
    "    # (عادة ما تحتوي على معلومات للتصحيح أو الإحصاءات، وليس لها استخدام مباشر في التدريب).\n",
    "    # truncated: تشير إلى ما إذا كانت الحلقة قد توقفت قبل تحقيق الهدف بسبب القيود (مثل عدد خطوات محدد).\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # إذا انتهت اللعبة، أعد تعيين البيئة\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ميثود أخرى مفيدة:\n",
    "\n",
    "step(action): لتطبيق إجراء في البيئة والحصول على الحالة الجديدة والمكافأة.\n",
    "\n",
    "step(action, reward): لتطبيق إجراء في البيئة والحصول على الحالة الجديده  والمكافأة.\n",
    "\n",
    "step(action, reward, done): لتطبيق إجراء في البيئة والحصول على الحالة  الجديدة والمكافأة و علامة انتهاء.\n",
    "\n",
    "step(action, reward, done, info): لتطبيق إجراء في البيئة والحصول على  الحالة الجديدة والمكافأة و علامة انتهاء و معلومات اضافية.\n",
    "\n",
    "action_space.sample(): للحصول على إجراء عشوائي صالح.\n",
    "\n",
    "render(): لعرض البيئة بشكل بصري.\n",
    "\n",
    "reset(): لإعادة تعيين البيئة إلى حالتها الأولية."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# إنشاء البيئة\n",
    "env = gym.make('MountainCar-v0',  render_mode='human')\n",
    "# عندما تقوم باستخدام render_mode=\"human\",\n",
    "# فإن البيئة ستعرض واجهة رسومية تفاعلية تسمح لك برؤية الحالة الحالية للبيئة في نافذة.\n",
    "# هذا العرض مفيد عندما تريد تتبع تقدم الوكيل بصريًا ومعرفة كيف يتفاعل مع البيئة بشكل مباشر.\n",
    "\n",
    "# إعادة تعيين البيئة\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "\n",
    "    # اختيار إجراء عشوائي\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # تنفيذ الإجراء في البيئة\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  # استيراد مكتبة gym التي تحتوي على العديد من البيئات المختلفة للتعلم المعزز\n",
    "\n",
    "# إنشاء بيئة MountainCar (السيارة الجبلية) من Gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# إعادة تعيين البيئة للحصول على الحالة الأولية، مما يعني بدء حلقة جديدة من التفاعل\n",
    "state = env.reset()\n",
    "\n",
    "# عرض الحالة الأولية للبيئة باستخدام واجهة رسومية (Render)، لرؤية السيارة وموقعها الحالي\n",
    "env.render()\n",
    "\n",
    "# تنفيذ إجراء عشوائي في البيئة. هنا، `env.action_space.sample()` يولد إجراءً عشوائيًا صالحًا \n",
    "result = env.step(env.action_space.sample())\n",
    "\n",
    "# تفكيك النتائج التي تم إرجاعها من `step()`. إذا تم إرجاع 5 قيم، يتم تعيينها على المتغيرات\n",
    "# إذا تم إرجاع 4 قيم فقط (في بعض الإصدارات القديمة)، يتم إضافة None كقيمة خامسة (truncated)\n",
    "state, reward, done, truncated, info = result if len(result) == 5 else (*result, None)\n",
    "\n",
    "# عرض الحالة الجديدة للبيئة بعد تنفيذ الإجراء، لمشاهدة تأثير الإجراء على البيئة\n",
    "env.render()\n",
    "\n",
    "# إغلاق البيئة بشكل صحيح لتحرير الموارد ومنع المشاكل مثل استمرار تشغيل النوافذ الرسومية\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مثال بسيط بدون استعمال بيئات جاهزه: الوكيل يتعلم التحرك في شبكة (2x2)\n",
    "\n",
    "الهدف:\n",
    "\n",
    "لدينا شبكة بسيطة (2x2)، \n",
    "والهدف هو أن يصل الوكيل إلى الموقع [1, 1] من الموقع [0, 0].\n",
    "\n",
    "إذا تحرك إلى [1, 1] يحصل على مكافأة = 10.\n",
    "\n",
    "كل حركة أخرى تكلفه -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "# أولا نقوم بتهيئة البيئة\n",
    "############################################################\n",
    "\n",
    "# تعريف الشبكة\n",
    "# هذه الشبكة تمثل البيئة التي يتنقل فيها الوكيل.\n",
    "# القيم في المصفوفة:\n",
    "# القيمة 0 تعني أن هذه الخلية في الشبكة لا تحتوي على مكافأة (أو قيمة المكافأة تساوي 0).\n",
    "# القيمة 10 تعني أن الخلية في هذا الموقع تحتوي على مكافأة قدرها 10.\n",
    "grid = [\n",
    "    [0, 0],\n",
    "    [0, 10]\n",
    "]\n",
    "\n",
    "# الحالات (State) تمثل المواقع\n",
    "states = [\n",
    "    (0, 0), \n",
    "    (0, 1), \n",
    "    (1, 0), \n",
    "    (1, 1)\n",
    "    ]\n",
    "\n",
    "# الإجراءات الممكنة\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "# تحديد الحدود\n",
    "def is_valid_move(state):\n",
    "    x, y = state\n",
    "    return 0 <= x < 2 and 0 <= y < 2\n",
    "# 0 <= x < 2: يعني أن قيمة x يجب أن تكون بين 0 و 1،\n",
    "# حيث 0 هو الحد الأدنى و 2 هو الحد الأقصى (ولكن غير شامل)،\n",
    "# أي أن القيم المقبولة لـ x\n",
    "# هي 0 أو 1.\n",
    "# 0 <= y < 2: نفس المفهوم ينطبق على y، حيث يجب أن تكون القيمة بين 0 و 1.\n",
    "# البيئة تحدد نتيجة الحركة\n",
    "\n",
    "def move(state, action):\n",
    "    x, y = state\n",
    "    if action == \"up\":\n",
    "        new_state = (x - 1, y)\n",
    "    elif action == \"down\":\n",
    "        new_state = (x + 1, y)\n",
    "    elif action == \"left\":\n",
    "        new_state = (x, y - 1)\n",
    "    elif action == \"right\":\n",
    "        new_state = (x, y + 1)\n",
    "    \n",
    "    # التأكد من أن الحركة صالحة\n",
    "    if is_valid_move(new_state):\n",
    "        return new_state\n",
    "    return state  # إذا كانت الحركة غير صالحة يبقى في نفس المكان\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# ثانيا نقوم بتهيئة الوكيل\n",
    "############################################################\n",
    "\n",
    "import random\n",
    "\n",
    "# الوكيل يتعلم باستخدام جدول Q-Learning\n",
    "# هو الجدول الذي يستخدمه الوكيل (Agent) لتخزين القيم المتعلقة بكل حالة وإجراء.\n",
    "# في التعلم المعزز، يتم استخدام Q-Table لتمثيل العلاقة بين الحالات (states) والإجراءات (actions)\n",
    "# مع توجيه الوكيل نحو أفضل إجراء يمكنه اتخاذه بناءً على خبراته السابقة.\n",
    "# 1. الحالات (States):\n",
    "# في هذه الحالة، الشبكة عبارة عن شبكة (2x2)\n",
    "# لذا لدينا 4 حالات: (0, 0), (0, 1), (1, 0), و(1, 1).\n",
    "#  كل حالة تمثل موقعًا مختلفًا في الشبكة.\n",
    "# 2. الإجراءات (Actions):\n",
    "# الوكيل يمكنه اتخاذ مجموعة من الإجراءات الممكنة: up, down, left, right.\n",
    "# 3. القيم (Values):\n",
    "# لكل حالة ولكل إجراء داخلها، توجد قيمة تمثل مدى جودة هذا الإجراء في هذه الحالة.\n",
    "#  في البداية، كل القيم تبدأ بـ 0، لأن الوكيل لم يتعلم بعد أي شيء عن البيئة.\n",
    "\n",
    "# الجدول الكامل q_table يبدو كما يلي في البداية:\n",
    "# {\n",
    "    # (0, 0): {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0},\n",
    "    # (0, 1): {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0},\n",
    "    # (1, 0): {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0},\n",
    "    # (1, 1): {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0}\n",
    "# }\n",
    "\n",
    "# يتم تحديث القيم في Table مع مرور الوقت باستخدام خوارزمية Q-Learning \n",
    "# بناءً على التجارب التي يخوضها الوكيل.\n",
    "q_table = {\n",
    "    (0, 0): {action: 0 for action in actions},\n",
    "    (0, 1): {action: 0 for action in actions},\n",
    "    (1, 0): {action: 0 for action in actions},\n",
    "    (1, 1): {action: 0 for action in actions}\n",
    "}\n",
    "\n",
    "\n",
    "# المعلمات\n",
    "alpha = 0.1  # معدل التعلم\n",
    "# قيمة alpha بين 0 و 1:\n",
    "# إذا كانت alpha قريبة من 1، فإن الوكيل يعتمد بشكل كبير على المعلومات الجديدة،\n",
    "#ويتعلم بسرعة (لكن قد يؤدي ذلك إلى تقلبات في القيم).\n",
    "# إذا كانت alpha قريبة من 0، فإن الوكيل يعتمد بشكل كبير على الخبرات القديمة ويتعلم ببطء.\n",
    "\n",
    "gamma = 0.9  # معامل الخصم\n",
    "# قيمة gamma بين 0 و 1:\n",
    "# إذا كانت gamma قريبة من 1، فإن الوكيل يولي اهتمامًا كبيرًا بالمكافآت المستقبلية.\n",
    "# إذا كانت gamma قريبة من 0، فإن الوكيل يهتم أكثر بالمكافآت الفورية.\n",
    "\n",
    "epsilon = 0.1  # معدل الاستكشاف\n",
    "# قيمة epsilon بين 0 و 1:\n",
    "# إذا كانت epsilon قريبة من 1، فإن الوكيل يستكشف البيئة أكثر (يتخذ إجراءات عشوائية في معظم الأحيان).\n",
    "# إذا كانت epsilon قريبة من 0، فإن الوكيل يستغل ما تعلمه أكثر \n",
    "# (يتخذ الإجراءات التي يعتبرها أفضل استنادًا إلى القيم الحالية في جدول Q).\n",
    "\n",
    "# اختيار إجراء باستخدام سياسة epsilon-greedy\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)  # استكشاف\n",
    "    return max(q_table[state], key=q_table[state].get)  # استغلال\n",
    "\n",
    "# تحديث جدول Q\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    best_next_action = max(q_table[next_state], key=q_table[next_state].get)\n",
    "    q_table[state][action] += alpha * (reward + gamma * q_table[next_state][best_next_action] - q_table[state][action])\n",
    "\n",
    "# المعادلة المكتوبة بشكل رياضي للتغير في الجدول:\n",
    "# α(reward + γ⋅maxQ(s′,a′) − Q(s,a)) + Q(s,a) ← Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# الأن نقوم بتجميع وتشغيل الخوارزمية\n",
    "############################################################\n",
    "\n",
    "# تجارب متعددة لتعلم الوكيل\n",
    "for episode in range(1000):\n",
    "    state = (0, 0)  # البداية دائماً من [0, 0]\n",
    "    \n",
    "    while state != (1, 1):  # تكرار حتى يصل الوكيل إلى الهدف\n",
    "        action = choose_action(state)\n",
    "        next_state = move(state, action)\n",
    "        \n",
    "        # تحديد المكافأة\n",
    "        reward = grid[next_state[0]][next_state[1]] if next_state == (1, 1) else -1\n",
    "        \n",
    "        # تحديث جدول Q\n",
    "        update_q_table(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state  # الانتقال إلى الحالة الجديدة\n",
    "\n",
    "print(\"The best  action for each state is:\")\n",
    "print(\"Q-Table after learning:\")\n",
    "for state, actions in q_table.items():\n",
    "    print(f\"State {state}: {actions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مفهوم PPO\n",
    "\n",
    "PPO هي خوارزمية تُستخدم لتحسين سياسة وكيل (Agent) في بيئة معينة.\n",
    "\n",
    "تقوم هذه الخوارزمية بتحديث السياسة الحالية للوكيل بطريقة تضمن عدم حدوث تغييرات كبيرة على السياسة الحالية،\n",
    "\n",
    "مما يساعد في تحقيق استقرار أفضل في عملية التدريب."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
