{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization**\n",
    "\n",
    "#### **1. ما هو Tokenization؟**\n",
    "---\n",
    "**Tokenization** هو الخطوة الأولى والأكثر أساسية في معالجة النصوص في مجال **معالجة اللغة الطبيعية (NLP)**. يهدف هذا المفهوم إلى تقسيم النصوص إلى أجزاء صغيرة تُعرف بـ \"tokens\" (الرموز) التي قد تكون كلمات، عبارات، أو حتى أحرف فردية، اعتمادًا على طريقة التقطيع.\n",
    "\n",
    "#### **2. لماذا نحتاج إلى Tokenization؟**\n",
    "---\n",
    "عند التعامل مع النصوص، يكون من الصعب على الخوارزميات فهم النص الكامل بدون تقسيمه إلى أجزاء صغيرة أو رموز يمكن معالجتها. يساعد Tokenization في تحويل النصوص إلى شكل يمكن فهمه بسهولة بواسطة الآلات لتحليلها أو استخدامها في نماذج التعلم الآلي.\n",
    "\n",
    "#### **3. أنواع Tokenization:**\n",
    "---\n",
    "هناك عدة طرق لتقطيع النص بناءً على حاجتنا:\n",
    "\n",
    "1. **Word Tokenization**: تقسيم النص إلى كلمات.\n",
    "2. **Character Tokenization**: تقسيم النص إلى أحرف فردية.\n",
    "3. **Subword Tokenization**: تقطيع النص إلى أجزاء صغيرة من الكلمات، وهو شائع في التعامل مع اللغات المعقدة أو النصوص التي تحتوي على كلمات غير مألوفة.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ziad.h.abaza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword: ['I', 'love', 'natural', 'language', 'processing', '!']\n",
      "character: ['I', ' ', 'l', 'o', 'v', 'e', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '!']\n",
      "Subword: ['i', 'love', 'natural', 'language', 'processing', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# جملة مثال\n",
    "sentence = \"I love natural language processing!\"\n",
    "\n",
    "# تقطيع إلى كلمات\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# عرض الرموز الناتجة\n",
    "print(f\"keyword: {tokens}\")\n",
    "\n",
    "# تقطيع إلى أحرف\n",
    "char_tokens = list(sentence)\n",
    "\n",
    "# عرض الرموز الناتجة\n",
    "print(f\"character: {char_tokens}\")\n",
    "\n",
    "# Subword Tokenization (تقطيع النص إلى أجزاء من الكلمات)\n",
    "# استخدام tokenizer من BERT أو أي نموذج آخر\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# تقطيع النص إلى أجزاء صغيرة\n",
    "subword_tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# عرض الرموز الناتجة\n",
    "print(f\"Subword: {subword_tokens}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lemmatisation**\n",
    "\n",
    "#### **1. ما هو Lemmatisation؟**\n",
    "---\n",
    "**Lemmatisation** هو عملية تحويل الكلمة إلى **شكلها الأساسي** أو ما يُعرف بـ \"lemma\"، وهي الشكل الأساسي للكلمة كما تظهر في القاموس. يختلف Lemmatisation عن **Stemming** (الاختزال) في أنه يحافظ على المعنى الدقيق للكلمة ويعيدها إلى شكلها الأساسي بناءً على القواعد النحوية للغة.\n",
    "\n",
    "#### **2. لماذا نحتاج إلى Lemmatisation؟**\n",
    "---\n",
    "الكلمات يمكن أن تتخذ أشكالًا متعددة (مثل الزمن الماضي، الجمع، صيغة الفاعل)، لذلك تهدف عملية **Lemmatisation** إلى تحويل هذه الأشكال المتعددة إلى الجذر أو الشكل الأساسي للكلمة، مما يساعد في تبسيط معالجة النصوص وتحليلها بشكل أدق. على سبيل المثال:\n",
    "\n",
    "- الكلمات: `running`, `ran`, `runs` تتحول إلى **lemma**: `run`.\n",
    "- الكلمات: `better`, `best` تتحول إلى **lemma**: `good`.\n",
    "\n",
    "#### **3. الفرق بين Lemmatisation و Stemming:**\n",
    "---\n",
    "- **Lemmatisation**: تعتمد على **القواعد النحوية** واللغة لإعادة الكلمة إلى أصلها، مما ينتج عنه كلمات مفهومة لغويًا.\n",
    "- **Stemming**: تعتمد على قواعد بسيطة لتقليص الكلمة إلى جذورها، وقد لا تكون النتيجة كلمة صحيحة (مثل: `running` قد تتحول إلى `run` أو `runn`).\n",
    "\n",
    "#### **4. كيف يتم استخدام Lemmatisation في NLP؟**\n",
    "---\n",
    "Lemmatisation تُستخدم بشكل واسع في تطبيقات معالجة اللغات الطبيعية مثل:\n",
    "- **تحليل النصوص**: تحويل الكلمات إلى جذورها لتحليل الكلمات المشتركة.\n",
    "- **البحث**: تحسين استعلامات البحث عن طريق مطابقة الكلمات المتنوعة مع أصلها.\n",
    "- **نماذج التعلم الآلي**: تقليل تعقيد البيانات وتحسين الدقة بتوحيد الكلمات المختلفة التي تعني الشيء نفسه.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'better', 'study', 'goose', 'fox', 'dog']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ziad.h.abaza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ziad.h.abaza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# تهيئة Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# أمثلة على الكلمات\n",
    "words = [\"running\", \"better\", \"studies\", \"geese\", \"foxes\", \"dogs\"]\n",
    "\n",
    "# تطبيق Lemmatisation\n",
    "lemmatized_words = []\n",
    "for word in words:\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "# عرض النتائج\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stop Words**\n",
    "\n",
    "#### **1. ما هي Stop Words؟**\n",
    "---\n",
    "**Stop Words** هي الكلمات الشائعة التي تظهر بشكل متكرر في النصوص لكنها غالبًا لا تحمل قيمة أو أهمية كبيرة في التحليل النصي. تشمل هذه الكلمات حروف الجر، العطف، والضمائر مثل:  \n",
    "- **في اللغة العربية**: \"و\"، \"في\"، \"على\"، \"من\".\n",
    "- **في اللغة الإنجليزية**: \"and\", \"the\", \"is\", \"in\".\n",
    "\n",
    "نظرًا لأن هذه الكلمات تظهر كثيرًا في النصوص، غالبًا ما يتم إزالتها أثناء التحليل لأن وجودها لا يضيف قيمة تحليلية أو معلوماتية للنص.\n",
    "\n",
    "#### **2. لماذا يتم إزالة Stop Words؟**\n",
    "---\n",
    "إزالة **Stop Words** مهمة لتحسين كفاءة التحليل وتقليل الضوضاء (Noise) في البيانات النصية. هذه الكلمات عادةً لا تساهم بشكل كبير في تحديد محتوى النص أو معناه، وإزالتها يساعد في:\n",
    "1. **تقليل حجم البيانات**: من خلال إزالة الكلمات غير الضرورية، يتم تقليل كمية البيانات التي يتم معالجتها.\n",
    "2. **تحسين أداء الخوارزميات**: يؤدي إلى تحليل أكثر دقة للنصوص المهمة مثل الكلمات الرئيسية أو العبارات المهمة.\n",
    "\n",
    "#### **3. أمثلة على Stop Words:**\n",
    "\n",
    "##### **3.1 في اللغة الإنجليزية:**\n",
    "- \"the\", \"is\", \"in\", \"at\", \"which\", \"on\", \"for\".\n",
    "\n",
    "##### **3.2 في اللغة العربية:**\n",
    "- \"الـ\"، \"من\"، \"على\"، \"إلى\"، \"في\"، \"و\"، \"أن\"، \"ما\".\n",
    "\n",
    "#### **كيف يتم استخدام Stop Words في NLP؟**\n",
    "---\n",
    "عادةً ما يتم إزالة **Stop Words** كخطوة مبكرة في معالجة النصوص قبل التحليل أو إنشاء نماذج التعلم الآلي. إليك كيفية تنفيذ ذلك باستخدام Python:\n",
    "\n",
    "#### **إزالة Stop Words في اللغة العربية:**\n",
    "---\n",
    "إزالة الكلمات الشائعة في اللغة العربية تحتاج إلى استخدام قائمة محددة لـ **Stop Words** باللغة العربية. يمكن استخدام مكتبات مثل `arabicstopwords` أو إنشاء قائمة مخصصة بالكلمات المراد إزالتها.\n",
    "\n",
    "\n",
    "\n",
    "#### **أهمية إزالة Stop Words في NLP:**\n",
    "---\n",
    "1. **زيادة الكفاءة**: تساعد إزالة Stop Words في تقليل حجم النصوص، مما يجعل معالجة البيانات أسرع وأكثر فعالية.\n",
    "2. **تحسين الدقة**: تركيز الخوارزميات على الكلمات ذات الأهمية يعزز من دقة التحليل والتنبؤ.\n",
    "3. **تقليل الضوضاء**: إزالة الكلمات غير المهمة يقلل من تشتت النماذج ويجعلها تركز على المحتوى الأساسي.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ziad.h.abaza\\AppData\\Roaming\\nltk_data..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: ['example', 'sentence', 'demonstrate', 'stop', 'words', 'removal', '.']\n",
      "Arabic: ['الطلاب', 'المدرسة', 'يحبون', 'اللعب', 'الساحة.']\n",
      "full EN: example sentence demonstrate stop words removal .\n",
      "full AR: الطلاب المدرسة يحبون اللعب الساحة.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# قائمة الكلمات الشائعة في اللغة العربية\n",
    "arabic_stop_words = [\"و\", \"في\", \"على\", \"من\", \"إلى\"]\n",
    "\n",
    "# جملة مثال\n",
    "sentence = \"This is an example sentence to demonstrate stop words removal.\"\n",
    "arabic_sentence = \"الطلاب في المدرسة يحبون اللعب في الساحة.\"\n",
    "\n",
    "# قائمة stop words باللغة الإنجليزية\n",
    "EN_words = set(stopwords.words('english'))\n",
    "# AR_words = set(stopwords.words('arabic'))\n",
    "\n",
    "# تقسيم الجملة إلى كلمات\n",
    "En_words = nltk.word_tokenize(sentence)\n",
    "AR_words = arabic_sentence.split()\n",
    "\n",
    "# إزالة stop words من الجملة الإنجليزية\n",
    "EN_filtered_sentence = []\n",
    "for word in En_words:\n",
    "    if word.lower() not in EN_words:\n",
    "        EN_filtered_sentence.append(word)\n",
    "\n",
    "# إزالة stop words من الجملة العربية\n",
    "AR_filtered_sentence = []\n",
    "for word in AR_words:\n",
    "    if word not in arabic_stop_words:\n",
    "        AR_filtered_sentence.append(word)\n",
    "\n",
    "# عرض الجملة بعد إزالة stop words\n",
    "print(f\"English: {EN_filtered_sentence}\")\n",
    "print(f\"Arabic: {AR_filtered_sentence}\")\n",
    "\n",
    "# عرض الجمله مجمعة\n",
    "full_EN_filtered_sentence = ' '.join(EN_filtered_sentence)\n",
    "print(f\"full EN: {full_EN_filtered_sentence}\")\n",
    "\n",
    "full_AR_filtered_sentence =  ' '.join(AR_filtered_sentence)\n",
    "print(f\"full AR: {full_AR_filtered_sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag of Words**\n",
    "\n",
    "#### **1. ما هو Bag of Words؟**\n",
    "---\n",
    "**Bag of Words (BoW)** هو نموذج بسيط وشائع يُستخدم لتمثيل النصوص في معالجة اللغات الطبيعية (NLP). يعتمد هذا النموذج على تحويل النصوص إلى **مجموعة من الكلمات** (أو الرموز)، مع تجاهل الترتيب النحوي للكلمات أو أي بنية لغوية أخرى. التركيز في هذا النموذج يكون فقط على **عدد تكرار الكلمات** في النص.\n",
    "\n",
    "#### **2. لماذا نستخدم Bag of Words؟**\n",
    "---\n",
    "نستخدم **Bag of Words** لتمثيل النصوص بطريقة تجعلها سهلة للمعالجة بواسطة الخوارزميات. بما أن النماذج الرياضية لا تستطيع فهم النصوص مباشرة، فإن BoW يقوم بتحويل النصوص إلى شكل عددي عن طريق:\n",
    "1. **تحليل التردد**: عدد المرات التي تظهر فيها كل كلمة في النص.\n",
    "2. **توحيد الهيكل**: تحويل النصوص إلى تمثيل يمكن معالجته عن طريق نماذج التعلم الآلي.\n",
    "\n",
    "#### **3. كيف يعمل Bag of Words؟**\n",
    "---\n",
    "**Bag of Words** يقوم بالخطوات التالية:\n",
    "1. **إنشاء قاموس من الكلمات**: يتم استخراج جميع الكلمات الفريدة (الرموز) التي تظهر في مجموعة النصوص.\n",
    "2. **عد التكرار**: لكل نص، يتم حساب عدد المرات التي تظهر فيها كل كلمة من القاموس.\n",
    "\n",
    "#### **مثال توضيحي:**\n",
    "\n",
    "لنفترض أن لدينا الجملتين التاليتين:\n",
    "- **الجملة 1**: \"أنا أحب البرمجة\".\n",
    "- **الجملة 2**: \"البرمجة ممتعة جدًا\".\n",
    "\n",
    "### **4. خطوات بناء Bag of Words:**\n",
    "\n",
    "#### **4.1. إنشاء قاموس الكلمات (Vocabulary):**\n",
    "نقوم بإنشاء قائمة تحتوي على جميع الكلمات الفريدة التي تظهر في كلا الجملتين:\n",
    "- القاموس: `[\"أنا\", \"أحب\", \"البرمجة\", \"ممتعة\", \"جدا\"]`\n",
    "\n",
    "#### **4.2. حساب التكرار (Frequency):**\n",
    "الآن نحسب عدد مرات ظهور كل كلمة في كل جملة:\n",
    "- **الجملة 1**: `\"أنا أحب البرمجة\"`\n",
    "  - أنا: 1\n",
    "  - أحب: 1\n",
    "  - البرمجة: 1\n",
    "  - ممتعة: 0\n",
    "  - جدا: 0\n",
    "\n",
    "- **الجملة 2**: `\"البرمجة ممتعة جدًا\"`\n",
    "  - أنا: 0\n",
    "  - أحب: 0\n",
    "  - البرمجة: 1\n",
    "  - ممتعة: 1\n",
    "  - جدا: 1\n",
    "\n",
    "نستطيع تمثيل هذه البيانات في شكل مصفوفة:\n",
    "\n",
    "|        | أنا | أحب | البرمجة | ممتعة | جدا |\n",
    "|--------|-----|-----|----------|--------|------|\n",
    "| جملة 1 |  1  |  1  |    1     |   0    |  0   |\n",
    "| جملة 2 |  0  |  0  |    1     |   1    |  1   |\n",
    "\n",
    "#### **كيفية تطبيق Bag of Words باستخدام Python:**\n",
    "\n",
    "نستخدم مكتبة `CountVectorizer` من `sklearn` لتنفيذ BoW على مجموعة نصوص.\n",
    "\n",
    "\n",
    "### **مميزات وعيوب Bag of Words:**\n",
    "\n",
    "#### **مميزات:**\n",
    "1. **بسيط وسهل الاستخدام**: BoW سهل التنفيذ والفهم.\n",
    "2. **غير معتمد على السياق**: لا يحتاج إلى أي معرفة حول ترتيب الكلمات.\n",
    "3. **فعال مع البيانات الصغيرة**: يعمل جيدًا مع مجموعات البيانات الصغيرة أو المتوسطة.\n",
    "\n",
    "#### **عيوب:**\n",
    "1. **تجاهل ترتيب الكلمات**: لا يحتفظ بمعلومات حول سياق الجملة أو ترتيب الكلمات.\n",
    "2. **ارتفاع أبعاد البيانات**: إذا كان القاموس يحتوي على الكثير من الكلمات، فإن مصفوفة BoW تصبح كبيرة جدًا.\n",
    "3. **عدم التعامل مع الكلمات المتكررة قليلًا**: BoW يتعامل مع كل الكلمات على قدم المساواة، حتى تلك التي قد لا تكون مهمة.\n",
    "\n",
    "#### **تحسينات على نموذج Bag of Words:**\n",
    "---\n",
    "لتجاوز بعض العيوب، يمكن استخدام نماذج محسنة مثل:\n",
    "1. **TF-IDF (Term Frequency - Inverse Document Frequency)**: يعطي وزنًا أكبر للكلمات المهمة التي تظهر كثيرًا في نص معين وأقل للكلمات الشائعة عبر كل النصوص.\n",
    "2. **Word Embeddings**: مثل **Word2Vec** أو **GloVe** التي تحافظ على السياق والمعاني المتضمنة في النص.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['أحب' 'أنا' 'البرمجة' 'جدا' 'ممتعة']\n",
      "Bag of Words Matrix:\n",
      " [[1 1 1 0 0]\n",
      " [0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# النصوص\n",
    "sentences = [\"أنا أحب البرمجة\", \"البرمجة ممتعة جدا\"]\n",
    "\n",
    "# إنشاء نموذج Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# تطبيق النموذج على النصوص\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# عرض القاموس\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# عرض مصفوفة BoW\n",
    "print(\"Bag of Words Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF**\n",
    "\n",
    "#### **1. ما هو TF-IDF؟**\n",
    "---\n",
    "**TF-IDF** هو اختصار لـ **Term Frequency - Inverse Document Frequency**، وهو تقنية تُستخدم في **تحليل النصوص** لتحويل النصوص إلى **تمثيلات عددية** تعتمد على **تكرار الكلمات** وأهميتها. يتم استخدامه لتحديد مدى أهمية كلمة معينة في **مستند نصي** معين مقارنة بمجموعة من المستندات (Corpus).\n",
    "\n",
    "يختلف عن نموذج **Bag of Words** لأنه لا يعتمد فقط على عدد تكرار الكلمة في النص، بل يأخذ في الحسبان مدى **ندرة أو شيوع** هذه الكلمة في مجموعة من النصوص الأخرى، مما يمنح تمثيلًا أدق للكلمات المهمة.\n",
    "\n",
    "#### **2. مكونات TF-IDF:**\n",
    "---\n",
    "يتكون **TF-IDF** من جزئين:\n",
    "1. **TF (Term Frequency)**: تكرار الكلمة في النص. يتم حسابه باستخدام المعادلة:\n",
    "   \\[\n",
    "   TF(t, d) = \\frac{\\text{عدد مرات تكرار الكلمة } t \\text{ في المستند } d}{\\text{إجمالي عدد الكلمات في المستند } d}\n",
    "   \\]\n",
    "   أي أنه يعبر عن نسبة تكرار الكلمة بالنسبة لباقي الكلمات في المستند.\n",
    "\n",
    "2. **IDF (Inverse Document Frequency)**: عكس تردد الكلمة في مجموعة المستندات. يتم حسابه باستخدام المعادلة:\n",
    "   \\[\n",
    "   IDF(t, D) = \\log\\left(\\frac{\\text{إجمالي عدد المستندات}}{\\text{عدد المستندات التي تحتوي على الكلمة } t}\\right)\n",
    "   \\]\n",
    "   وهذا يعني أن الكلمات الشائعة جدًا في مجموعة النصوص ستحصل على قيمة IDF منخفضة، بينما الكلمات النادرة تحصل على قيمة أعلى.\n",
    "\n",
    "#### **3. معادلة TF-IDF:**\n",
    "---\n",
    "لحساب **TF-IDF** لكلمة معينة، يتم ضرب القيمتين **TF** و **IDF** معًا:\n",
    "\\[\n",
    "TF\\text{-}IDF(t, d, D) = TF(t, d) \\times IDF(t, D)\n",
    "\\]\n",
    "\n",
    "#### **4. لماذا نستخدم TF-IDF؟**\n",
    "---\n",
    "1. **تحسين دقة التحليل**: يُستخدم TF-IDF لتمييز الكلمات المهمة في المستندات وتقليل تأثير الكلمات الشائعة جدًا (مثل Stop Words).\n",
    "2. **إعطاء وزن للكلمات**: TF-IDF يعطي وزنًا أعلى للكلمات التي تظهر بشكل متكرر في مستند معين ولكنها نادرة في المستندات الأخرى، مما يساعد على فهم محتوى المستند بشكل أدق.\n",
    "3. **تحسين نماذج تعلم الآلة**: يمكن استخدام تمثيل النصوص باستخدام TF-IDF كمدخلات لنماذج تعلم الآلة لتحليل النصوص وتصنيفها.\n",
    "\n",
    "#### **مثال على استخدام TF-IDF:**\n",
    "\n",
    "لنفترض أن لدينا الجملتين التاليتين:\n",
    "- الجملة 1: `\"I love natural language processing.\"`\n",
    "- الجملة 2: `\"Language processing is fascinating.\"`\n",
    "\n",
    "### **حساب TF و IDF لكل كلمة**\n",
    "- **TF** لكل كلمة يعبر عن نسبة تكرار الكلمة بالنسبة لباقي الكلمات في الجملة.\n",
    "- **IDF** يعتمد على مدى ندرة الكلمة في مجموعة المستندات.\n",
    "\n",
    "### **تطبيق TF-IDF باستخدام Python:**\n",
    "\n",
    "يمكننا استخدام مكتبة **scikit-learn** لحساب قيم TF-IDF بسهولة.\n",
    "\n",
    "\n",
    "#### **مميزات TF-IDF:**\n",
    "---\n",
    "1. **تمييز الكلمات الهامة**: يعطي قيمة أعلى للكلمات التي تكون فريدة أو نادرة في مجموعة النصوص، مما يساعد في فهم المحتوى الأساسي للنص.\n",
    "2. **تقليل تأثير الكلمات الشائعة**: يقلل من تأثير الكلمات الشائعة التي لا تضيف معلومات مفيدة مثل \"is\" و \"the\".\n",
    "3. **تمثيل نصوص فعال**: يوفر تمثيلًا عدديًا يمكن استخدامه كمدخلات لنماذج تعلم الآلة لتحليل النصوص.\n",
    "\n",
    "#### **سلبيات TF-IDF:**\n",
    "---\n",
    "1. **إهمال ترتيب الكلمات**: مثل نموذج **Bag of Words**، لا يأخذ **TF-IDF** ترتيب الكلمات في الحسبان، مما يعني أنه لا يعالج المعلومات السياقية.\n",
    "2. **غير فعال مع النصوص الطويلة**: قد يصبح حساب **TF-IDF** معقدًا أو غير عملي عند التعامل مع نصوص طويلة جدًا.\n",
    "3. **عدم القدرة على التعامل مع المعاني المتعددة للكلمة**: لا يمكن لـ **TF-IDF** تمييز معاني الكلمات المختلفة إذا كانت نفس الكلمة تُستخدم بمعاني مختلفة في سياقات متعددة.\n",
    "\n",
    "#### **تطبيقات TF-IDF:**\n",
    "---\n",
    "- **تحليل النصوص**: يُستخدم لتحليل النصوص الكبيرة أو المقالات الطويلة لتحديد الكلمات المفتاحية.\n",
    "- **محركات البحث**: تحسين نتائج البحث من خلال تحديد مدى ارتباط كلمة معينة بصفحة معينة.\n",
    "- **تحليل المشاعر**: يمكن استخدامه لتحديد الكلمات الأكثر أهمية والتي تعبر عن مشاعر إيجابية أو سلبية.\n",
    "- **تصنيف النصوص**: في نماذج تعلم الآلة مثل تصنيف البريد الإلكتروني أو تصنيف المقالات.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'lorem': 69, 'ipsum': 62, 'dolor': 30, 'sit': 118, 'amet': 6, 'consectetur': 15, 'adipisicing': 3, 'elit': 41, 'quo': 104, 'soluta': 119, 'culpa': 20, 'eos': 43, 'porro': 93, 'quaerat': 96, 'dolorum': 36, 'enim': 42, 'cupiditate': 23, 'dolorem': 32, 'voluptatibus': 134, 'fuga': 54, 'recusandae': 107, 'aspernatur': 9, 'aliquam': 5, 'nihil': 80, 'vero': 130, 'animi': 7, 'repellendus': 110, 'molestias': 74, 'voluptates': 133, 'asperiores': 8, 'sequi': 115, 'nostrum': 84, 'ad': 2, 'cumque': 22, 'cum': 21, 'eligendi': 40, 'facere': 52, 'unde': 125, 'nobis': 82, 'eaque': 38, 'magnam': 70, 'sed': 114, 'dicta': 28, 'ullam': 124, 'ex': 49, 'nam': 76, 'harum': 56, 'natus': 77, 'quis': 102, 'non': 83, 'eum': 47, 'excepturi': 50, 'dignissimos': 29, 'tempora': 120, 'corporis': 18, 'iusto': 65, 'atque': 11, 'magni': 71, 'reprehenderit': 111, 'at': 10, 'minus': 72, 'perferendis': 90, 'fugiat': 55, 'nesciunt': 79, 'similique': 116, 'alias': 4, 'aut': 12, 'veritatis': 129, 'inventore': 60, 'tenetur': 122, 'optio': 88, 'pariatur': 89, 'eius': 39, 'dolores': 34, 'corrupti': 19, 'modi': 73, 'odit': 85, 'officiis': 86, 'consequatur': 16, 'quidem': 101, 'dolore': 31, 'ratione': 106, 'quisquam': 103, 'perspiciatis': 91, 'mollitia': 75, 'voluptatum': 135, 'deleniti': 26, 'doloribus': 35, 'doloremque': 33, 'omnis': 87, 'laboriosam': 66, 'incidunt': 59, 'nisi': 81, 'ipsam': 61, 'praesentium': 94, 'est': 45, 'quibusdam': 100, 'voluptatem': 132, 'repellat': 109, 'error': 44, 'sint': 117, 'et': 46, 'beatae': 14, 'delectus': 25, 'eveniet': 48, 'consequuntur': 17, 'vel': 126, 'quas': 98, 'necessitatibus': 78, 'debitis': 24, 'velit': 127, 'totam': 123, 'accusantium': 1, 'facilis': 53, 'veniam': 128, 'placeat': 92, 'deserunt': 27, 'sapiente': 113, 'reiciendis': 108, 'ab': 0, 'temporibus': 121, 'autem': 13, 'impedit': 58, 'saepe': 112, 'ducimus': 37, 'itaque': 63, 'vitae': 131, 'iure': 64, 'quam': 97, 'laudantium': 67, 'quod': 105, 'quasi': 99, 'libero': 68, 'hic': 57, 'provident': 95, 'explicabo': 51}\n",
      "TF-IDF Matrix:\n",
      " [[0.08192319 0.0409616  0.08192319 0.0409616  0.0409616  0.12288479\n",
      "  0.20480798 0.08192319 0.16384638 0.12288479 0.08192319 0.12288479\n",
      "  0.08192319 0.08192319 0.08192319 0.12288479 0.0409616  0.16384638\n",
      "  0.0409616  0.08192319 0.0409616  0.12288479 0.16384638 0.08192319\n",
      "  0.0409616  0.08192319 0.08192319 0.12288479 0.08192319 0.0409616\n",
      "  0.12288479 0.0409616  0.12288479 0.08192319 0.08192319 0.08192319\n",
      "  0.0409616  0.0409616  0.12288479 0.12288479 0.0409616  0.0409616\n",
      "  0.12288479 0.0409616  0.0409616  0.0409616  0.0409616  0.0409616\n",
      "  0.08192319 0.0409616  0.16384638 0.0409616  0.12288479 0.08192319\n",
      "  0.12288479 0.0409616  0.08192319 0.0409616  0.0409616  0.0409616\n",
      "  0.0409616  0.0409616  0.08192319 0.08192319 0.0409616  0.0409616\n",
      "  0.0409616  0.0409616  0.0409616  0.0409616  0.0409616  0.08192319\n",
      "  0.12288479 0.08192319 0.28673117 0.0409616  0.08192319 0.12288479\n",
      "  0.08192319 0.0409616  0.0409616  0.0409616  0.08192319 0.0409616\n",
      "  0.0409616  0.08192319 0.08192319 0.12288479 0.0409616  0.08192319\n",
      "  0.0409616  0.08192319 0.08192319 0.16384638 0.08192319 0.0409616\n",
      "  0.08192319 0.0409616  0.0409616  0.08192319 0.0409616  0.08192319\n",
      "  0.12288479 0.0409616  0.08192319 0.0409616  0.0409616  0.0409616\n",
      "  0.12288479 0.0409616  0.0409616  0.0409616  0.0409616  0.0409616\n",
      "  0.08192319 0.16384638 0.08192319 0.08192319 0.08192319 0.0409616\n",
      "  0.08192319 0.0409616  0.08192319 0.08192319 0.12288479 0.08192319\n",
      "  0.0409616  0.0409616  0.0409616  0.08192319 0.0409616  0.0409616\n",
      "  0.0409616  0.08192319 0.08192319 0.12288479]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# الجملتين\n",
    "sentences = [\"Lorem ipsum dolor sit amet, consectetur adipisicing elit. Dolor, quo. Soluta culpa eos porro quaerat, dolorum enim cupiditate dolorem! Voluptatibus fuga recusandae aspernatur aliquam nihil, vero animi repellendus molestias voluptates, asperiores sequi, porro nostrum ad cumque cum. Eligendi facere unde nobis eaque magnam enim sed dicta, ullam ex nam molestias cumque harum natus quis, non eum excepturi amet. Eaque dignissimos nam, tempora corporis iusto atque magni reprehenderit at sequi minus porro perferendis fugiat nesciunt. Amet dolor similique cumque alias aut natus veritatis aspernatur inventore tenetur ullam fuga enim optio asperiores pariatur molestias eius, sequi, dolores corrupti modi odit. Officiis consequatur quidem voluptates dolore excepturi ratione quisquam, unde perspiciatis molestias, atque mollitia minus quo voluptatum deleniti doloribus odit doloremque, omnis laboriosam asperiores incidunt nisi. Ipsam, praesentium! Est eius asperiores, voluptatum quibusdam dolorem ullam voluptatem repellat sit nobis error porro sint cum atque, quidem omnis et! Fuga, veritatis excepturi, beatae facere quis aliquam dolorem delectus eveniet eius aut consequuntur vel quas cum tempora sint necessitatibus debitis velit consectetur ipsum totam molestias, natus accusantium. Quaerat, facilis! Excepturi veniam, placeat cupiditate deserunt sapiente, facere reiciendis consequuntur, perspiciatis quis sequi ab temporibus similique autem omnis impedit molestias? Consequuntur, totam deserunt delectus voluptatum modi beatae deleniti necessitatibus saepe magni corrupti ducimus. Itaque dolores, tenetur, sed at ad reiciendis facilis aliquam vitae iure praesentium officiis eaque! Aspernatur, autem pariatur. Quam amet consectetur laudantium cumque quod doloremque doloribus quasi voluptatibus reiciendis? Animi, placeat ab harum deserunt eveniet consequuntur, dicta itaque molestias minus libero hic amet provident quasi explicabo?\"]\n",
    "\n",
    "# تحويل الجمل إلى أحرف صغيرة\n",
    "sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "# تهيئة TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# تحويل الجمل إلى قيم TF-IDF\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# عرض المفردات\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# عرض مصفوفة TF-IDF\n",
    "print(\"TF-IDF Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Unigram**\n",
    "\n",
    "#### **1. ما هو Unigram؟**\n",
    "---\n",
    "**Unigram** هو أبسط شكل من أشكال **n-grams** في معالجة اللغات الطبيعية. في هذا النموذج، يتم تقسيم النص إلى وحدات صغيرة تتكون من **كلمة واحدة فقط**. يتم النظر إلى كل كلمة في النص على أنها وحدة مستقلة عن الكلمات الأخرى، ولا يتم الاهتمام بسياق الكلمة أو الكلمات المجاورة لها.\n",
    "\n",
    "#### **2. لماذا نستخدم Unigram؟**\n",
    "---\n",
    "نستخدم **Unigram** كنموذج أساسي لتحليل النصوص في التطبيقات مثل تصنيف النصوص، حيث:\n",
    "- **يبسط** عملية معالجة النصوص عن طريق التركيز على الكلمات الفردية فقط.\n",
    "- **يساعد في فهم تكرار الكلمات** الأساسية في النص دون الاهتمام بترتيبها أو الروابط بينها.\n",
    "\n",
    "#### **3. كيف يعمل Unigram؟**\n",
    "---\n",
    "في نموذج **Unigram**، نقوم بتقسيم النص إلى كلمات فردية. إذا كان لدينا نص يحتوي على جملة، فإن كل كلمة في الجملة تعتبر Unigram.\n",
    "\n",
    "#### **مثال على Unigram:**\n",
    "\n",
    "لنفترض أن لدينا الجملة التالية:\n",
    "- **\"أنا أحب البرمجة\"**\n",
    "\n",
    "نقوم بتقسيم الجملة إلى Unigrams:\n",
    "- **أنا**\n",
    "- **أحب**\n",
    "- **البرمجة**\n",
    "\n",
    "في هذا النموذج، كل كلمة تعتبر وحدة مستقلة.\n",
    "\n",
    "#### **كيفية استخدام Unigram باستخدام Python:**\n",
    "\n",
    "يمكننا استخدام مكتبة `CountVectorizer` في `sklearn` لتطبيق Unigram، حيث أنها تقوم افتراضيًا بإنشاء Unigrams عند تحويل النصوص إلى بيانات عددية.\n",
    "\n",
    "\n",
    "### **مميزات وعيوب Unigram:**\n",
    "\n",
    "#### **مميزات:**\n",
    "- **البساطة**: سهل التنفيذ والفهم.\n",
    "- **الكفاءة**: مناسب للنصوص الصغيرة والمتوسطة، حيث أن التعامل مع الكلمات الفردية يُبسط الحسابات.\n",
    "\n",
    "#### **عيوب:**\n",
    "- **عدم الأخذ في الاعتبار السياق**: لا يحتفظ Unigram بأي معلومات حول سياق الكلمات أو ترتيبها في الجملة.\n",
    "- **فقدان المعاني المركبة**: في بعض الأحيان، قد تحتوي الجمل على تعابير مكونة من أكثر من كلمة (مثل الأفعال المركبة أو التعابير الاصطلاحية) التي لا يمكن لنموذج Unigram التعرف عليها.\n",
    "\n",
    "### **استخدامات Unigram في معالجة اللغات الطبيعية (NLP)**\n",
    "\n",
    "### **تصنيف النصوص (Text Classification)**\n",
    "- في تصنيف النصوص، يتم تحويل النصوص إلى تمثيلات عددية باستخدام Unigram لتحديد الكلمات الأكثر شيوعًا في النصوص المختلفة. يتم تدريب النماذج على هذه الكلمات لتحديد الفئات أو التصنيفات المناسبة.\n",
    "- **مثال**: تصنيف الرسائل الإلكترونية إلى رسائل مهمة أو غير مهمة (Spam/Not Spam).\n",
    "\n",
    "### **استخراج الكلمات المفتاحية (Keyword Extraction)**\n",
    "- يستخدم Unigram لاستخراج الكلمات الأكثر تكرارًا في النصوص والتي غالبًا ما تكون الكلمات المفتاحية التي تلخص محتوى النص.\n",
    "- **مثال**: في المقالات أو الأخبار، يمكن استخراج الكلمات الأساسية لتلخيص المحتوى أو إنشاء الكلمات الدلالية (tags).\n",
    "\n",
    "### **إنشاء حقائب الكلمات (Bag of Words)**\n",
    "- Unigram هو الخطوة الأولى في إنشاء نموذج **Bag of Words**، حيث يتم تقسيم النصوص إلى كلمات منفردة ثم يتم تحويلها إلى مصفوفة تمثل تكرار كل كلمة في النص.\n",
    "- **مثال**: عند التعامل مع وثائق متعددة، يتم إنشاء تمثيل عددي لكل وثيقة بناءً على الكلمات الفردية الموجودة فيها باستخدام Unigram.\n",
    "\n",
    "### **تصحيح الأخطاء الإملائية (Spell Correction)**\n",
    "- يمكن استخدام Unigram لتحديد الكلمات الشائعة والأقل شيوعًا في مجموعة كبيرة من النصوص. الكلمات غير الشائعة يمكن أن تشير إلى أخطاء إملائية يتم تصحيحها بناءً على الكلمات الأكثر شيوعًا.\n",
    "- **مثال**: أنظمة التصحيح التلقائي تعتمد على Unigram لاكتشاف الكلمات غير الصحيحة وتصحيحها.\n",
    "\n",
    "### **إنشاء نماذج لغوية (Language Models)**\n",
    "- يستخدم Unigram لبناء نماذج لغوية بسيطة تعتمد على تحليل الكلمات الفردية وتكرارها. يتم استخدام هذه النماذج لتوقع الكلمات التالية في النص بناءً على تكرار الكلمات في البيانات التدريبية.\n",
    "- **مثال**: في تطبيقات الكتابة التنبؤية (Predictive Text)، يتم استخدام Unigram لتوقع الكلمات الأكثر احتمالاً استنادًا إلى الكلمات السابقة.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['أحب' 'أنا' 'البرمجة' 'ممتعة']\n",
      "Unigram Matrix:\n",
      " [[1 1 1 0]\n",
      " [0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# النصوص\n",
    "sentences = [\"أنا أحب البرمجة\", \"البرمجة ممتعة\"]\n",
    "\n",
    "# إنشاء نموذج Unigram باستخدام CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))  # Ngram (1, 1) تعني Unigram\n",
    "\n",
    "# تطبيق النموذج على النصوص\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# عرض القاموس (المفردات)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# عرض مصفوفة Unigram\n",
    "print(\"Unigram Matrix:\\n\", X.toarray())\n",
    "\n",
    "# ngram_range هو معلمة تستخدم في مكتبة scikit-learn،\n",
    "# وتحديدًا في CountVectorizer و TfidfVectorizer، لتحديد نطاق الـ n-grams الذي تريد تضمينه عند تحويل النصوص إلى تمثيل عددي.\n",
    "\n",
    "# n-grams هي تسلسلات مكونة من n كلمات متتابعة في النص. على سبيل المثال:\n",
    "\n",
    "# Unigram (1-gram): \"أنا أحب البرمجة\" -> [\"أنا\", \"أحب\", \"البرمجة\"]\n",
    "# Bigram (2-gram): \"أنا أحب البرمجة\" -> [\"أنا أحب\", \"أحب البرمجة\"]\n",
    "# Trigram (3-gram): \"أنا أحب البرمجة\" -> [\"أنا أحب البرمجة\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bigram**\n",
    "\n",
    "#### **1. ما هو Bigram؟**\n",
    "---\n",
    "**Bigram** هو نموذج من نماذج **n-grams** في معالجة اللغات الطبيعية (NLP) حيث يتم تحليل النص إلى **سلاسل مكونة من كلمتين** متتاليتين. بمعنى آخر، يتم تقسيم النص إلى أزواج متتابعة من الكلمات. \n",
    "\n",
    "في نموذج Bigram، يُؤخذ في الاعتبار ترتيب الكلمات والعلاقة بين كل زوج من الكلمات في النص. هذا يعكس المعلومات السياقية التي قد تكون غائبة في نموذج Unigram الذي يعامل الكلمات بشكل مستقل.\n",
    "\n",
    "#### **2. لماذا نستخدم Bigram؟**\n",
    "---\n",
    "نستخدم **Bigram** لتوفير مزيد من المعلومات حول سياق الكلمات والعلاقات بينها، مما يساعد في تحسين فهم النصوص وتطبيقات مثل:\n",
    "- **تحليل النصوص**: لفهم كيفية ظهور الكلمات معًا.\n",
    "- **التنبؤ بالكلمات**: لتحسين النماذج مثل التنبؤ بالكلمات في لوحات المفاتيح.\n",
    "- **نموذج اللغة**: لتحسين الترجمة الآلية وتوليد النصوص.\n",
    "\n",
    "#### **3. مثال على Bigram:**\n",
    "---\n",
    "لنفترض أن لدينا الجملة التالية:\n",
    "\n",
    "- **\"أنا أحب البرمجة\"**\n",
    "\n",
    "إذا قمنا بتطبيق Bigram عليها، سنحصل على الأزواج التالية:\n",
    "\n",
    "- [\"أنا أحب\", \"أحب البرمجة\"]\n",
    "\n",
    "كل زوج من الكلمات يعبر عن العلاقة بين الكلمات المتجاورة في النص.\n",
    "\n",
    "#### **4. تطبيق Bigram باستخدام Python:**\n",
    "\n",
    "يمكننا استخدام مكتبة `CountVectorizer` من `sklearn` مع تحديد `ngram_range` لتطبيق Bigram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams Vocabulary: ['أحب البرمجة' 'أنا أحب' 'البرمجة ممتعة' 'ممتعة جدا']\n",
      "Bigram Matrix:\n",
      " [[1 1 0 0]\n",
      " [0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# النصوص\n",
    "sentences = [\"أنا أحب البرمجة\", \"البرمجة ممتعة جدا\"]\n",
    "\n",
    "# تهيئة CountVectorizer مع ngram_range=(2, 2) لتطبيق Bigram\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# تحويل الجمل إلى تمثيل Bigram\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# عرض المفردات (الـ Bigrams)\n",
    "print(\"Bigrams Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# عرض مصفوفة Bigram\n",
    "print(\"Bigram Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regular Expressions**\n",
    "\n",
    "#### **1. ما هي Regular Expressions؟**\n",
    "---\n",
    "**التعبيرات النمطية** أو **Regular Expressions (Regex)** هي نمط أو صيغة محددة تُستخدم للبحث، المطابقة، أو التحقق من النصوص. تمكنك من العمل مع النصوص بطريقة ديناميكية ومرنة عبر تحديد أنماط معقدة للعثور على أو تعديل أجزاء من النص. تُستخدم Regular Expressions بشكل شائع في معالجة النصوص، البرمجة، وتنقية البيانات.\n",
    "\n",
    "#### **2. لماذا نستخدم Regular Expressions؟**\n",
    "---\n",
    "- **البحث عن نص معين**: مثل العثور على أرقام هواتف أو عناوين بريد إلكتروني داخل مستند نصي.\n",
    "- **التحقق من مدخلات المستخدم**: مثل التأكد من أن عنوان البريد الإلكتروني مكتوب بشكل صحيح أو التحقق من كلمة المرور.\n",
    "- **تعديل النصوص**: مثل استبدال كلمات أو عبارات معينة بنص آخر.\n",
    "\n",
    "#### **3. بناء Regular Expressions:**\n",
    "---\n",
    "التعبيرات النمطية تتكون من **رموز خاصة** و**أحرف** تُستخدم لتحديد الأنماط. بعض الرموز الخاصة الأكثر استخدامًا تشمل:\n",
    "\n",
    "- `.`: يطابق أي حرف مفرد (باستثناء الأسطر الجديدة).\n",
    "- `^`: يطابق بداية السطر.\n",
    "- `$`: يطابق نهاية السطر.\n",
    "- `*`: يطابق الحرف أو النمط السابق 0 أو أكثر من المرات.\n",
    "- `+`: يطابق الحرف أو النمط السابق 1 أو أكثر من المرات.\n",
    "- `?`: يجعل النمط السابق اختياريًا (يطابق مرة واحدة أو لا يطابق).\n",
    "- `{n}`: يطابق النمط السابق تمامًا **n** من المرات.\n",
    "- `[abc]`: يطابق أي حرف من الأحرف داخل الأقواس.\n",
    "- `|`: يعمل كـ OR بين الأنماط.\n",
    "\n",
    "#### **4. أمثلة على Regular Expressions:**\n",
    "---\n",
    "**مثال 1: البحث عن أرقام الهواتف**\n",
    "\n",
    "- تعبير نمطي للعثور على رقم هاتف في صيغة: `123-456-7890`\n",
    "\n",
    "```regex\n",
    "\\d{3}-\\d{3}-\\d{4}\n",
    "```\n",
    "- **شرح**:\n",
    "  - `\\d`: يطابق أي رقم.\n",
    "  - `{3}`: يطابق 3 أرقام متتالية.\n",
    "  - `-`: يطابق علامة `-`.\n",
    "\n",
    "**مثال 2: التحقق من عنوان بريد إلكتروني**\n",
    "\n",
    "- تعبير نمطي للتحقق من صحة عنوان بريد إلكتروني:\n",
    "\n",
    "```regex\n",
    "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\n",
    "```\n",
    "\n",
    "- **شرح**:\n",
    "  - `[a-zA-Z0-9._%+-]+`: يطابق الأحرف أو الأرقام أو الرموز المسموح بها في عنوان البريد الإلكتروني.\n",
    "  - `@`: يطابق رمز الـ \"@\".\n",
    "  - `[a-zA-Z0-9.-]+`: يطابق اسم النطاق.\n",
    "  - `\\.`: يطابق النقطة.\n",
    "  - `[a-zA-Z]{2,}`: يطابق نطاق مثل \".com\" أو \".net\" يتكون من حرفين أو أكثر.\n",
    "\n",
    "#### **5. تطبيق Regular Expressions باستخدام Python:**\n",
    "\n",
    "في Python، يمكننا استخدام مكتبة `re` للتعامل مع التعبيرات النمطية.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email:  ['example@mail.com']\n",
      "phone:  ['123-456-7890']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"my email is: example@mail.com and my phone is: 123-456-7890\"\n",
    "\n",
    "\n",
    "# البحث عن البريد الإلكتروني\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "email = re.findall(email_pattern, text)\n",
    "\n",
    "# البحث عن رقم الهاتف\n",
    "phone_pattern = r'\\d{3}-\\d{3}-\\d{4}'\n",
    "phone = re.findall(phone_pattern, text)\n",
    "\n",
    "print(\"email: \", email)\n",
    "print(\"phone: \", phone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part-of-Speech (POS) Tagging using SpaCy**\n",
    "\n",
    "#### **1. ما هو Part-of-Speech (POS) Tagging؟**\n",
    "---\n",
    "**POS Tagging** أو **توسيم أجزاء الكلام** هو عملية تصنيف كل كلمة في النص بناءً على دورها النحوي، مثل الفعل، الاسم، الصفة، وغيرها. هذا يساعد في فهم بنية الجملة وتحليل النصوص بشكل أعمق.\n",
    "\n",
    "#### **2. لماذا نستخدم POS Tagging؟**\n",
    "---\n",
    "- **تحليل النصوص**: يساعد في تحليل كيفية تفاعل الكلمات مع بعضها في الجملة.\n",
    "- **فهم السياق**: يساعد في تحديد معاني الكلمات بناءً على السياق.\n",
    "- **تحسين التطبيقات اللغوية**: مثل الترجمة الآلية وتحليل المشاعر.\n",
    "\n",
    "#### **3. كيفية استخدام SpaCy لتصنيف أجزاء الكلام:**\n",
    "---\n",
    "**SpaCy** هو مكتبة قوية في Python لمعالجة اللغة الطبيعية والتي توفر أدوات متقدمة لـ POS Tagging.\n",
    "\n",
    "#### **الخطوة 1: تثبيت SpaCy وتنزيل نموذج لغوي**\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "# or\n",
    "python -m spacy download en_core_web_md\n",
    "# or\n",
    "python -m spacy download en_core_web_lg\n",
    "```\n",
    "\n",
    "\n",
    "#### **4. مميزات وعيوب POS Tagging باستخدام SpaCy:**\n",
    "\n",
    "##### **مميزات:**\n",
    "1. **دقة عالية**: يقدم SpaCy نتائج دقيقة وقابلة للتطبيق على نصوص متنوعة.\n",
    "2. **سهل الاستخدام**: واجهة المستخدم بسيطة وسهلة الفهم.\n",
    "3. **تحليل سريع**: يدعم التحليل السريع للنصوص الكبيرة.\n",
    "\n",
    "##### **عيوب:**\n",
    "1. **تطلب موارد**: يمكن أن يكون التحليل كثيفًا من حيث الموارد، خاصة مع النصوص الكبيرة.\n",
    "2. **نموذج واحد**: النماذج اللغوية قد لا تكون دقيقة في جميع الحالات، خاصة للغات غير الإنجليزية.\n",
    "\n",
    "### **خلاصة:**\n",
    "---\n",
    "**POS Tagging** باستخدام SpaCy هو أداة قوية لتصنيف أجزاء الكلام في النصوص. تساعد هذه الأداة في تحليل بنية الجمل وفهم دور الكلمات في السياق. SpaCy يقدم واجهة سهلة الاستخدام ونتائج دقيقة، مما يجعله خيارًا ممتازًا في معالجة اللغة الطبيعية."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: SpaCy, POS: PROPN, Tag: NNP, Lemma: SpaCy\n",
      "Word: is, POS: AUX, Tag: VBZ, Lemma: be\n",
      "Word: an, POS: DET, Tag: DT, Lemma: an\n",
      "Word: open, POS: ADJ, Tag: JJ, Lemma: open\n",
      "Word: -, POS: PUNCT, Tag: HYPH, Lemma: -\n",
      "Word: source, POS: NOUN, Tag: NN, Lemma: source\n",
      "Word: library, POS: NOUN, Tag: NN, Lemma: library\n",
      "Word: for, POS: ADP, Tag: IN, Lemma: for\n",
      "Word: advanced, POS: ADJ, Tag: JJ, Lemma: advanced\n",
      "Word: NLP, POS: PROPN, Tag: NNP, Lemma: NLP\n",
      "Word: in, POS: ADP, Tag: IN, Lemma: in\n",
      "Word: Python, POS: PROPN, Tag: NNP, Lemma: Python\n",
      "Word: ., POS: PUNCT, Tag: ., Lemma: .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# تحميل النموذج اللغوي\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# النص\n",
    "text = \"SpaCy is an open-source library for advanced NLP in Python.\"\n",
    "\n",
    "# تطبيق تحليل النصوص\n",
    "doc = nlp(text)\n",
    "\n",
    "# عرض أجزاء الكلام\n",
    "for token in doc:\n",
    "    print(f\"Word: {token.text}, POS: {token.pos_}, Tag: {token.tag_}, Lemma: {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Named Entity Recognition (NER)**\n",
    "\n",
    "#### **1. ما هو Named Entity Recognition (NER)؟**\n",
    "---\n",
    "**NER** أو **التعرف على الكيانات المسماة** هو عملية تحديد واستخراج الكيانات ذات الأهمية الخاصة من النص، مثل الأسماء (أشخاص، شركات، بلدان)، المواقع الجغرافية، التواريخ، الأرقام، وغيرها من الكيانات المعروفة. يتيح هذا النوع من التحليل فهم النصوص بشكل أعمق من خلال التركيز على العناصر الأساسية الموجودة في الجملة.\n",
    "\n",
    "#### **2. لماذا نستخدم NER؟**\n",
    "---\n",
    "- **تحليل النصوص بشكل أكثر دقة**: يساعد في تحديد المعلومات الهامة مثل الأشخاص، الأماكن، والتواريخ.\n",
    "- **تحليل المعلومات الآلية**: يفيد في التطبيقات مثل محركات البحث، أنظمة الترجمة الآلية، وتحليل الأخبار.\n",
    "- **تنظيم البيانات**: يساعد في تصنيف وتنظيم المعلومات غير المنظمة من النصوص الكبيرة.\n",
    "\n",
    "#### **NER using SpaCy:**\n",
    "---\n",
    "**SpaCy** يقدم أداة قوية وسهلة الاستخدام للتعرف على الكيانات المسماة من النصوص.\n",
    "\n",
    "#### **4. التصنيفات الأساسية في NER:**\n",
    "---\n",
    "- **PERSON**: أشخاص (أسماء أفراد)\n",
    "- **ORG**: منظمات (شركات، جمعيات)\n",
    "- **GPE**: الأماكن (بلدان، مدن)\n",
    "- **DATE**: تواريخ\n",
    "- **MONEY**: مبالغ مالية\n",
    "- **LOC**: مواقع جغرافية\n",
    "\n",
    "\n",
    "### **خلاصة:**\n",
    "---\n",
    "**NER** هو أداة قوية لاستخراج المعلومات الأساسية من النصوص. **SpaCy** يعتبر الخيار الأمثل للمستخدمين الذين يبحثون عن أداة سريعة وسهلة الاستخدام، بينما يمكن أن تكون **NLTK** خيارًا جيدًا للبحث الأكاديمي والتخصيص."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# تحميل النموذج اللغوي قم بتحميله بواسطة terminal اولا\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# النص\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion on June 3rd, 2023. joan me i'm ziad hassan, \"\n",
    "\n",
    "# تحليل النصوص\n",
    "doc = nlp(text)\n",
    "\n",
    "# استخراج الكيانات المسماة\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **spell correction with textblob and spwllchecker**\n",
    "\n",
    "### **خلاصة:**\n",
    "---\n",
    "تصحيح الأخطاء الإملائية هو جزء مهم من معالجة النصوص، ويمكن تحقيقه باستخدام أدوات مثل **TextBlob** و**SpellChecker**. TextBlob يقدم واجهة سهلة مع مجموعة من الأدوات الإضافية، بينما SpellChecker يوفر دقة عالية في تصحيح الأخطاء. اختيار الأداة المناسبة يعتمد على متطلبات المشروع والتفضيلات الشخصية.\n",
    "\n",
    "#### **TextBlob**\n",
    "\n",
    "**TextBlob** هي مكتبة Python لمعالجة اللغة الطبيعية توفر أدوات سهلة الاستخدام لتصحيح الأخطاء الإملائية.\n",
    "\n",
    "##### **تثبيت TextBlob**\n",
    "```bash\n",
    "pip install textblob\n",
    "```\n",
    "\n",
    "#### **SpellChecker **\n",
    "\n",
    "**SpellChecker** هي مكتبة Python توفر أدوات لتصحيح الأخطاء الإملائية واكتشاف الكلمات غير الصحيحة.\n",
    "\n",
    "##### **تثبيت SpellChecker**\n",
    "```bash\n",
    "pip install pyspellchecker\n",
    "```\n",
    "\n",
    "#### **4. مميزات وعيوب TextBlob وSpellChecker:**\n",
    "\n",
    "##### **مميزات TextBlob:**\n",
    "1. **سهل الاستخدام**: واجهة بسيطة وسهلة الاستخدام.\n",
    "2. **وظائف متعددة**: يوفر أدوات أخرى مثل التحليل العاطفي والتصحيح النحوي.\n",
    "\n",
    "##### **عيوب TextBlob:**\n",
    "1. **دقة محدودة**: قد تكون الدقة أقل مقارنة بالأدوات الأخرى في بعض الحالات.\n",
    "\n",
    "##### **مميزات SpellChecker:**\n",
    "1. **دقة عالية**: يقدم دقة عالية في تصحيح الأخطاء الإملائية.\n",
    "2. **قابلية التخصيص**: يمكن تخصيص القاموس بناءً على احتياجات المستخدم.\n",
    "\n",
    "##### **عيوب SpellChecker:**\n",
    "1. **واجهات أقل**: يركز فقط على تصحيح الأخطاء الإملائية ولا يتضمن أدوات إضافية.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I have a speling error in this sentense.\n",
      "Corrected Text: I have a spelling error in this sentence.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# النص مع الأخطاء الإملائية\n",
    "text = \"I have a speling error in this sentense.\"\n",
    "\n",
    "# إنشاء كائن TextBlob\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# تصحيح الأخطاء الإملائية\n",
    "corrected_text = blob.correct()\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Corrected Text:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I havve a speling error in this sentense.\n",
      "Corrected Text: I have a spieling error in this sentenced\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# النص مع الأخطاء الإملائية\n",
    "text = \"I havve a speling error in this sentense.\"\n",
    "\n",
    "# إنشاء كائن SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# تقسيم النص إلى كلمات\n",
    "words = text.split()\n",
    "\n",
    "# تصحيح الأخطاء الإملائية لكل كلمة\n",
    "corrected_words = [spell.candidates(word).pop() if spell.candidates(word) else word for word in words]\n",
    "corrected_text = ' '.join(corrected_words)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Corrected Text:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Embedding**\n",
    "\n",
    "#### **1. ما هو Word Embedding؟**\n",
    "---\n",
    "**Word Embedding** هو تمثيل الكلمات بشكل عددي في فضاء متعدد الأبعاد (vectors) بحيث تكون الكلمات ذات المعاني المتشابهة قريبة من بعضها في هذا الفضاء. الهدف من هذه العملية هو تمثيل الكلمات بطريقة تسهل على الحاسوب فهم العلاقات المعنوية بينها.\n",
    "\n",
    "#### **2. لماذا نستخدم Word Embedding؟**\n",
    "---\n",
    "- **التمثيل العددي**: يتيح لنا تمثيل الكلمات كـ vectors مما يجعل التعامل معها في الشبكات العصبية وغيرها من خوارزميات التعلم الآلي ممكناً.\n",
    "- **فهم المعاني**: يساعد على فهم العلاقات المعنوية بين الكلمات؛ مثلاً، كلمات مثل \"king\" و\"queen\" تكون قريبة في الفضاء لأنها ذات معنى متشابه.\n",
    "- **التعلم العميق**: Word Embeddings تستخدم كمدخلات في الشبكات العصبية لتطبيقات مثل معالجة اللغة الطبيعية (NLP).\n",
    "\n",
    "#### **كيفية استخدام طبقة Embedding في Keras لإنشاء Word Embeddings**\n",
    "\n",
    "**Keras** هو إطار عمل قوي لتطوير الشبكات العصبية، ويوفر طبقة خاصة لإنشاء Word Embeddings.\n",
    "\n",
    "\n",
    "#####  **شرح Embedding Layer في Keras**\n",
    "---\n",
    "**Embedding Layer** في Keras هي الطبقة التي تحول الكلمات إلى تمثيل عددّي (vectors). هذه الطبقة تأخذ الكلمات وتحولها إلى vectors بأبعاد معينة.\n",
    "\n",
    "##### **أهم معايير Embedding Layer:**\n",
    "- **input_dim**: حجم المفردات (عدد الكلمات الفريدة).\n",
    "- **output_dim**: الأبعاد التي سيتم تمثيل كل كلمة بها.\n",
    "- **input_length**: طول التسلسل (عدد الكلمات في الجملة).\n",
    "\n",
    "### معاملات `Embedding` الأساسية:\n",
    "1. **`vocab_size`**: عدد الكلمات الفريدة (المفردات) في مجموعة البيانات. يمثل هذا المعامل حجم المفردات التي نريد تمثيلها باستخدام الـ embeddings.\n",
    "   \n",
    "   - على أي أساس نحدد القيمة؟\n",
    "     - هذه القيمة تعتمد على حجم البيانات النصية وعدد الكلمات الفريدة التي يتم استخدامها. غالباً، نحدد هذه القيمة بناءً على عدد الكلمات الأكثر شيوعًا في مجموعة البيانات (أي بعد إزالة الكلمات النادرة جدًا).\n",
    "     - يتم اختيارها بعد معالجة البيانات واستخراج المفردات الفريدة.\n",
    "\n",
    "2. **`embedding_dim`**: هو عدد الأبعاد في المتجه الذي يمثل كل كلمة. هذا المتغير يحدد حجم المتجه الذي نريد أن يمثّل كل كلمة.\n",
    "   \n",
    "   - على أي أساس نحدد القيمة؟\n",
    "     - القيم الشائعة: يمكن أن تكون 50، 100، 200 أو 300. كلما زاد العدد، زاد تمثيل الكلمات بدقة أكبر ولكن زادت أيضًا المتطلبات الحسابية.\n",
    "     - إذا كانت البيانات معقدة وتحتوي على العديد من العلاقات المختلفة بين الكلمات، من الأفضل استخدام قيمة أكبر لـ `embedding_dim`.\n",
    "\n",
    "3. **`input_length`**: طول تسلسل الكلمات (أي عدد الكلمات في الجملة أو النص).\n",
    "   \n",
    "   - على أي أساس نحدد القيمة؟\n",
    "     - يتم تحديده بناءً على الحد الأقصى لطول النصوص في مجموعة البيانات. على سبيل المثال، إذا كانت أطول جملة في مجموعة البيانات تحتوي على 100 كلمة، فإن `input_length` يمكن أن تكون 100.\n",
    "     - إذا كانت النصوص تختلف بشكل كبير في الطول، يمكن تحديد طول ثابت وتطبيق padding لإضافة أو إزالة كلمات لضمان أن جميع الجمل تحتوي على نفس العدد من الكلمات.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "import numpy as np\n",
    "\n",
    "# تعريف قيم المثال\n",
    "vocab_size = 20  # عدد الكلمات الفريدة + 1\n",
    "embedding_dim = 8  # عدد الأبعاد\n",
    "input_length = 5  # طول الجملة\n",
    "\n",
    "# بناء النموذج باستخدام Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# إضافة Embedding Layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "\n",
    "# تحويل المصفوفة الناتجة إلى شكل واحد\n",
    "model.add(Flatten())\n",
    "\n",
    "# إضافة طبقة Dense لتصنيف النصوص (كمثال بسيط)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ملخص النموذج\n",
    "model.summary()\n",
    "\n",
    "# بيانات تجريبية (جمل مشفرة بالأرقام)\n",
    "input_data = np.array([[1, 2, 3, 0, 0], # \"I love AI\"\n",
    "                       [3, 4, 5, 0, 0], #AI is amazing\"\n",
    "                       [6, 2, 7, 0, 0], # \"its love me\"\n",
    "                       [1, 8, 9, 0, 0], #i like pizza\"\n",
    "                       [10, 11, 12, 13, 0],]) # my name is ali  \n",
    "                       \n",
    "\n",
    "# التنبؤ باستخدام النموذج\n",
    "output = model.predict(input_data)\n",
    "print(\"Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word2Vec - GloVe**\n",
    "\n",
    "#### **1. ما هو Word2Vec؟**\n",
    "---\n",
    "**Word2Vec** هو نموذج تم تطويره بواسطة Google يُستخدم لتحويل الكلمات إلى متجهات عددية (vectors) بحيث يتم تمثيل كل كلمة كمتجه ذو أبعاد ثابتة. هذه المتجهات تحافظ على المعلومات الدلالية والعلاقات بين الكلمات.\n",
    "\n",
    "##### **كيف يعمل Word2Vec؟**\n",
    "- **النموذجين الرئيسيين:** \n",
    "  - **Continuous Bag of Words (CBOW):** يتنبأ بالكلمة المستهدفة من سياقها (الكلمات المحيطة بها).\n",
    "  - **Skip-gram:** يتنبأ بالكلمات المحيطة بكلمة مستهدفة.\n",
    "  \n",
    "\n",
    "#### **2. ما هو GloVe؟**\n",
    "---\n",
    "**GloVe (Global Vectors for Word Representation)** هو نموذج تم تطويره بواسطة جامعة ستانفورد لتحويل الكلمات إلى متجهات عددية. يركز على استغلال الإحصاءات العالمية للكلمات من خلال بناء مصفوفة تكرار الكلمات.\n",
    "\n",
    "##### **كيف يعمل GloVe؟**\n",
    "- **استغلال إحصاءات التكرار:** يعتمد على تحليل الإحصاءات العالمية لتكرار الكلمات والبيانات التكرارية بين الكلمات.\n",
    "- **التمثيل المترابط:** يحاول تقليل الفجوة بين التمثيلات المستخرجة من التكرار الفعلي للكلمات والتكرار المتوقع بين الكلمات.\n",
    "\n",
    "#### **مميزات وعيوب:**\n",
    "\n",
    "##### **مميزات Word2Vec:**\n",
    "1. **دقة عالية:** يقدم تمثيلات دقيقة للعلاقات بين الكلمات.\n",
    "2. **تدريب سريع:** يعمل بشكل جيد مع البيانات الكبيرة.\n",
    "\n",
    "##### **عيوب Word2Vec:**\n",
    "1. **تحتاج إلى بيانات كبيرة:** للحصول على نتائج جيدة، يتطلب نموذج Word2Vec نصوص ضخمة.\n",
    "2. **يعمل فقط مع سياق الكلمات:** التركيز الأساسي هو سياق الكلمات المباشر.\n",
    "\n",
    "##### **مميزات GloVe:**\n",
    "1. **الاستفادة من الإحصاءات العالمية:** يوفر تمثيلات دقيقة بناءً على الإحصاءات العامة.\n",
    "2. **فهم دلالي أعمق:** يستفيد من البيانات الإحصائية العالمية للحصول على دلالات أعمق.\n",
    "\n",
    "##### **عيوب GloVe:**\n",
    "1. **تدريب طويل:** قد يكون أبطأ في التدريب مقارنة بـ Word2Vec.\n",
    "2. **تطلب معالجة معقدة:** يحتاج إلى بناء مصفوفة التكرار وحسابات إحصائية معقدة.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# مجموعة بيانات تجريبية\n",
    "sentences = [\n",
    "    ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
    "    ['the', 'dog', 'barked'],\n",
    "    ['the', 'cat', 'and', 'the', 'dog', 'played']\n",
    "]\n",
    "\n",
    "# تدريب نموذج Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# الحصول على تمثيل الكلمة\n",
    "vector = model.wv['cat']\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تحميل نموذج GloVe مُدرّب مسبقًا\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# تحميل المتجهات من ملف\n",
    "glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False)\n",
    "# download from https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "# الحصول على تمثيل الكلمة\n",
    "vector = glove_vectors['cat']\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN => NLP**\n",
    "\n",
    "#### **لماذا نستخدم RNN في معالجة اللغة الطبيعية؟**\n",
    "---\n",
    "اللغة تعتمد على السياق؛ أي أن معنى الكلمة أو الجملة يعتمد على الكلمات والجمل السابقة. **RNN** تسمح للنماذج بفهم السياق عن طريق تذكر المعلومات السابقة، مما يجعلها مثالية للتعامل مع النصوص واللغات الطبيعية.\n",
    "\n",
    "##### **تطبيقات RNN في NLP:**\n",
    "- **تحليل المشاعر:** فهم ما إذا كان النص يعبر عن مشاعر إيجابية أو سلبية بناءً على الكلمات السابقة.\n",
    "- **ترجمة الآلية:** استخدام RNN لترجمة النصوص من لغة إلى أخرى.\n",
    "- **التلخيص النصي:** تلخيص النصوص الطويلة مع الاحتفاظ بالمعلومات الهامة.\n",
    "- **التعرف على الكلام:** تحويل الكلام المنطوق إلى نص.\n",
    "\n",
    "#### **3. كيف تعمل RNN؟**\n",
    "---\n",
    "**RNN** تعتمد على فكرة التكرار. عندما تمرر البيانات عبر RNN، تأخذ الوحدة العصبية المدخل الحالي (input) بالإضافة إلى حالتها السابقة (previous hidden state) لإنتاج الإخراج الحالي (output) والحالة الحالية (current hidden state). وهكذا تستمر السلسلة مع كل مدخل جديد.\n",
    "\n",
    "##### **مكونات RNN الأساسية:**\n",
    "- **المدخلات (Input):** تمثل البيانات النصية أو التسلسل الزمني.\n",
    "- **الحالة المخفية (Hidden State):** تمثل المعلومات المخزنة من المدخلات السابقة.\n",
    "- **الإخراج (Output):** يتم إنتاجه بناءً على المدخل الحالي والحالة المخفية.\n",
    "\n",
    "##### **التكرار عبر الزمن:**\n",
    "في كل خطوة زمنية (time step):\n",
    "- يأخذ النموذج المدخل الحالي.\n",
    "- يجمعه مع الحالة المخفية السابقة.\n",
    "- ينتج حالة مخفية جديدة وإخراج جديد.\n",
    "\n",
    "#### **4. مشكلات RNN التقليدية:**\n",
    "---\n",
    "رغم قوة RNN، فإنها تواجه مشكلات عند التعامل مع البيانات المتسلسلة الطويلة، مثل:\n",
    "- **اختفاء التدرج (Vanishing Gradient Problem):** يحدث عندما تصبح التدرجات صغيرة جدًا خلال عملية التدريب باستخدام الـ backpropagation، مما يجعل الشبكة غير قادرة على تعلم العلاقات الطويلة الأمد.\n",
    "- **التذكر قصير الأمد:** RNN التقليدية تجد صعوبة في تذكر المعلومات على مدى طويل في السلسلة.\n",
    "\n",
    "#### **5. حلول مشكلات RNN:**\n",
    "---\n",
    "لتفادي هذه المشكلات، تم تطوير نماذج محسّنة مثل:\n",
    "\n",
    "##### **1. Long Short-Term Memory (LSTM):**\n",
    "- **LSTM** هو نوع من RNN يحتوي على وحدات ذاكرة يمكنها تذكر المعلومات لفترات زمنية طويلة. يتكون LSTM من بوابات للتحكم في تدفق المعلومات:\n",
    "  - **بوابة الإدخال (Input Gate):** تقرر ما هي المعلومات الجديدة التي سيتم تخزينها.\n",
    "  - **بوابة النسيان (Forget Gate):** تقرر ما هي المعلومات القديمة التي سيتم نسيانها.\n",
    "  - **بوابة الإخراج (Output Gate):** تتحكم في ما سيتم إخراجه بناءً على الحالة المخفية.\n",
    "\n",
    "##### **2. Gated Recurrent Unit (GRU):**\n",
    "- **GRU** هو نسخة مبسطة من LSTM، حيث يحتوي على بوابتين فقط:\n",
    "  - **بوابة التحديث (Update Gate):** تتحكم في كمية المعلومات التي سيتم تحديثها.\n",
    "  - **بوابة النسيان (Reset Gate):** تتحكم في كمية المعلومات التي سيتم نسيانها."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "import numpy as np\n",
    "\n",
    "# الجملة التي سنعمل عليها\n",
    "sentence = \"أنا أحب تعلم البرمجة باستخدام بايثون\"\n",
    "\n",
    "# تحويل الجملة إلى قائمة كلمات منفصلة\n",
    "words = sentence.split()\n",
    "\n",
    "###############################################################\n",
    "# نقوم بإنشاء تسلسلات من الكلمات بحيث نبدأ بالكلمة الأولى\n",
    "# ثم نضيف الكلمة التالية لتكوين سلسلة أطول حتى نهاية الجملة.\n",
    "# كل تسلسل يحتوي على الكلمات السابقة والكلمة الحالية.\n",
    "###############################################################\n",
    "sequences = []\n",
    "for i in range(1, len(words)):\n",
    "    sequence = words[:i+1]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# عرض التسلسلات الناتجة\n",
    "for seq in sequences:\n",
    "    print(seq)\n",
    "\n",
    "# تهيئة المحول (Tokenizer) لتقوم بترميز الكلمات إلى أرقام\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# تدريب المحول على الكلمات في الجملة\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "\n",
    "# تحويل التسلسلات النصية إلى تسلسلات من الأرقام\n",
    "encoded_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "###############################################################\n",
    "# نقوم بتحديد الطول الأقصى للتسلسلات لضمان أن جميعها متساوية الطول.\n",
    "# ثم نستخدم وظيفة pad_sequences لتوسيط التسلسلات الأقصر عن طريق\n",
    "# إضافة أصفار في البداية لضمان أن جميع التسلسلات بنفس الطول.\n",
    "###############################################################\n",
    "max_len = max(len(seq) for seq in encoded_sequences)\n",
    "padded_sequences = pad_sequences(encoded_sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "# تحويل التسلسلات إلى مدخلات (X) ومخرجات (y).\n",
    "# X هي الكلمات السابقة، و y هي الكلمة التي يجب التنبؤ بها.\n",
    "X, y = padded_sequences[:, :-1], padded_sequences[:, -1]\n",
    "\n",
    "# عرض البيانات المدخلة (X) والمخرجات (y)\n",
    "print(\"المدخلات:\\n\", X)\n",
    "print(\"المخرجات:\\n\", y)\n",
    "\n",
    "###############################################################\n",
    "# نقوم بحساب حجم المفردات (عدد الكلمات الفريدة) عن طريق \n",
    "# عدد الكلمات التي تم تدريب المحول عليها.\n",
    "###############################################################\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "###############################################################\n",
    "# بناء النموذج العصبي\n",
    "# الطبقة الأولى: Embedding لتحويل الأرقام إلى تمثيلات شعاعية (vectors)\n",
    "# الطبقة الثانية: SimpleRNN لمعالجة التتابعات\n",
    "# الطبقة الثالثة: Dense للخروج والتنبؤ بالكلمة التالية.\n",
    "###############################################################\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1))  # طبقة الترميز\n",
    "model.add(SimpleRNN(50))  # طبقة RNN لمعالجة التسلسل\n",
    "model.add(Dense(vocab_size, activation='softmax'))  # طبقة إخراج للتنبؤ بالكلمة\n",
    "\n",
    "###############################################################\n",
    "# نقوم بتجميع النموذج باستخدام optimizer (adam) \n",
    "# وخسارة من نوع sparse_categorical_crossentropy لبيانات التصنيف.\n",
    "###############################################################\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "###############################################################\n",
    "# تدريب النموذج على البيانات (X, y) لعدد معين من الدورات (epochs)\n",
    "# هنا نستخدم 500 دورة تدريبية لتحسين النموذج.\n",
    "###############################################################\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "\n",
    "###############################################################\n",
    "# دالة للتنبؤ بالكلمة التالية\n",
    "# تقوم هذه الدالة بأخذ النص وتحويله إلى تسلسل من الأرقام،\n",
    "# ثم تقوم بتوسيط التسلسل لتتناسب مع الطول المطلوب.\n",
    "# يتم التنبؤ بالكلمة التالية باستخدام النموذج المدرب.\n",
    "###############################################################\n",
    "def predict_next_word(model, tokenizer, text, max_len):\n",
    "    sequence = tokenizer.texts_to_sequences([text.split()])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len-1, padding='pre')\n",
    "    predicted = model.predict(padded_sequence, verbose=0)\n",
    "    predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "    return predicted_word\n",
    "\n",
    "###############################################################\n",
    "# نستخدم الدالة للتنبؤ بالكلمة التالية بعد \"أنا أحب\"\n",
    "###############################################################\n",
    "print(predict_next_word(model, tokenizer, \"تعلم\", max_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM => NLP**\n",
    "\n",
    "####  **المشكلة مع الشبكات العصبية المتكررة التقليدية (RNNs)**\n",
    "الشبكات العصبية المتكررة التقليدية تعاني من مشكلة **الانفجار والانفجار المفرط** في التدرجات أثناء التدريب، مما يجعل من الصعب عليها تعلم الأنماط على المدى الطويل. هذه المشكلة تعود إلى كيفية تمرير المعلومات من خطوة زمنية إلى أخرى.\n",
    "\n",
    "#### **حل LSTM لمشاكل RNN التقليدية**\n",
    "تحتوي وحدة LSTM على **وحدات ذاكرة** يمكنها الاحتفاظ بالمعلومات على المدى الطويل. ولتحقيق ذلك، تستخدم LSTM ما يسمى بـ **بوابات** (Gates) التي تتحكم في تدفق المعلومات عبر الشبكة. هذه البوابات تشمل:\n",
    "\n",
    "- **بوابة الإدخال (Input Gate)**: تتحكم في مقدار المعلومات الجديدة التي يجب إدخالها إلى الذاكرة.\n",
    "- **بوابة النسيان (Forget Gate)**: تحدد مقدار المعلومات التي يجب نسيانها من الذاكرة.\n",
    "- **بوابة الإخراج (Output Gate)**: تتحكم في مقدار المعلومات المخزنة في الذاكرة التي يجب استخدامها في الحسابات التالية.\n",
    "\n",
    "#### **آلية عمل LSTM**\n",
    "- **الإدخال**: البيانات النصية يتم تحويلها إلى تمثيلات عددية (مثل الكلمات المضمنة) وتغذيتها في LSTM.\n",
    "- **الذاكرة**: LSTM تستخدم الذاكرة للحفاظ على المعلومات عبر خطوات زمنية متعددة، مما يساعد في فهم السياق الطويل.\n",
    "- **التنبؤ**: بعد معالجة التسلسل من خلال عدة خطوات، يمكن لنموذج LSTM تقديم تنبؤات بناءً على المعلومات المخزنة في الذاكرة.\n",
    "\n",
    "### تطبيقات LSTM في NLP\n",
    "\n",
    "1. **ترجمة الآلة**: استخدام LSTM لترجمة النصوص من لغة إلى أخرى عبر فهم تسلسل الكلمات وسياقها.\n",
    "2. **تحليل المشاعر**: تصنيف النصوص بناءً على مشاعرها (إيجابي، سلبي، محايد).\n",
    "3. **توليد النصوص**: إنشاء نصوص جديدة بناءً على سياق النصوص المدخلة.\n",
    "4. **التعرف على الكلام**: تحويل الصوت إلى نصوص من خلال فهم تسلسل الأصوات.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# جمل تجريبية\n",
    "sentences = [\n",
    "    'I love machine learning',\n",
    "    'I love deep learning',\n",
    "    'I enjoy coding in Python',\n",
    "    'Machine learning is fun',\n",
    "    'Deep learning is powerful'\n",
    "]\n",
    "\n",
    "# إعداد Tokenizer لتحويل الكلمات إلى أرقام\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# تحويل الجمل إلى تسلسلات عددية\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# إعداد البيانات للتدريب\n",
    "input_sequences = []\n",
    "next_words = []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        input_sequences.append(seq[:i])\n",
    "        next_words.append(seq[i])\n",
    "\n",
    "# ملء المصفوفات لتصبح بنفس الطول\n",
    "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = [seq + [0] * (max_sequence_length - len(seq)) for seq in input_sequences]\n",
    "\n",
    "X = np.array(input_sequences)\n",
    "y = to_categorical(next_words, num_classes=total_words)\n",
    "\n",
    "###############################################################\n",
    "# بناء نموذج LSTM\n",
    "###############################################################\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_length))  # طبقة Embedding لتحويل الكلمات إلى تمثيلات عددية\n",
    "model.add(LSTM(50))  # 50 وحدة LSTM\n",
    "model.add(Dense(total_words, activation='softmax'))  # طبقة Dense للتنبؤ بالكلمات\n",
    "\n",
    "# تجميع النموذج\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "###############################################################\n",
    "# تدريب النموذج\n",
    "###############################################################\n",
    "model.fit(X, y, epochs=50, verbose=1)\n",
    "\n",
    "###############################################################\n",
    "# توقع الكلمة التالية\n",
    "###############################################################\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_length):\n",
    "    # تحويل النص إلى تسلسل عددية\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = sequence[-max_sequence_length:]  # أخذ آخر خطوات زمنية فقط\n",
    "    sequence = np.array(sequence).reshape(1, -1)\n",
    "    \n",
    "    # التنبؤ بالكلمة التالية\n",
    "    predicted_probs = model.predict(sequence, verbose=0)[0]\n",
    "    predicted_word_index = np.argmax(predicted_probs)\n",
    "    \n",
    "    # تحويل التوقع إلى كلمة\n",
    "    word_index = tokenizer.word_index\n",
    "    index_word = {index: word for word, index in word_index.items()}\n",
    "    \n",
    "    return index_word.get(predicted_word_index, 'Unknown')\n",
    "\n",
    "# اختبار التنبؤ\n",
    "input_text = 'I love'\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)\n",
    "print(f'Predicted next word: {predicted_word}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الـ **GRU** (Gated Recurrent Unit) هو نوع من الشبكات العصبية المتكررة (RNNs) ويعتبر بديلاً لـ LSTM (Long Short-Term Memory) في معالجة اللغة الطبيعية (NLP). تم تقديم GRU كتحسين للـ RNNs التقليدية بهدف التغلب على مشاكل الـ RNNs مثل الصعوبات في تعلم الأنماط على المدى الطويل.\n",
    "\n",
    "### كيف تعمل GRU في NLP؟\n",
    "\n",
    "#### 1. **المشكلة مع الشبكات العصبية المتكررة التقليدية (RNNs)**\n",
    "الشبكات العصبية المتكررة التقليدية تواجه صعوبة في الحفاظ على المعلومات على مدى فترات زمنية طويلة. وهذا يسبب مشاكل في التعلم عندما تحتاج الشبكة إلى الاحتفاظ بالمعلومات لفترات طويلة لتوقع الأنماط.\n",
    "\n",
    "#### **حل GRU لمشاكل RNN التقليدية**\n",
    "تستخدم GRU ما يسمى بـ **البوابات** (Gates) للتحكم في تدفق المعلومات، ولكنها تستخدم عددًا أقل من البوابات مقارنةً بـ LSTM، مما يجعلها أبسط وأسرع في التدريب. البوابات الرئيسية في GRU هي:\n",
    "\n",
    "- **بوابة التحديث (Update Gate)**: تحدد مقدار المعلومات القديمة التي يجب الاحتفاظ بها ومقدار المعلومات الجديدة التي يجب إضافتها.\n",
    "- **بوابة إعادة التعيين (Reset Gate)**: تتحكم في مقدار المعلومات القديمة التي يجب نسيانها عند حساب الحالة الجديدة.\n",
    "\n",
    "#### **آلية عمل GRU**\n",
    "- **الإدخال**: يتم تحويل البيانات النصية إلى تمثيلات عددية (مثل الكلمات المضمنة).\n",
    "- **الذاكرة**: يتم استخدام البوابات للتحكم في تدفق المعلومات وحفظ السياق الطويل.\n",
    "- **التنبؤ**: بعد معالجة التسلسل من خلال عدة خطوات، يمكن لنموذج GRU تقديم التنبؤات بناءً على المعلومات المخزنة.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
    "\n",
    "# جمل تجريبية\n",
    "sentences = [\n",
    "    'I love machine learning',\n",
    "    'I love deep learning',\n",
    "    'I enjoy coding in Python',\n",
    "    'Machine learning is fun',\n",
    "    'Deep learning is powerful'\n",
    "]\n",
    "\n",
    "# إعداد Tokenizer لتحويل الكلمات إلى أرقام\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# تحويل الجمل إلى تسلسلات عددية\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# إعداد البيانات للتدريب\n",
    "input_sequences = []\n",
    "next_words = []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        input_sequences.append(seq[:i])\n",
    "        next_words.append(seq[i])\n",
    "\n",
    "# ملء المصفوفات لتصبح بنفس الطول\n",
    "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = [seq + [0] * (max_sequence_length - len(seq)) for seq in input_sequences]\n",
    "\n",
    "X = np.array(input_sequences)\n",
    "y = to_categorical(next_words, num_classes=total_words)\n",
    "\n",
    "###############################################################\n",
    "# بناء نموذج GRU\n",
    "###############################################################\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_length))  # طبقة Embedding لتحويل الكلمات إلى تمثيلات عددية\n",
    "model.add(GRU(50))  # 50 وحدة GRU\n",
    "model.add(Dense(total_words, activation='softmax'))  # طبقة Dense للتنبؤ بالكلمات\n",
    "\n",
    "# تجميع النموذج\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "###############################################################\n",
    "# تدريب النموذج\n",
    "###############################################################\n",
    "model.fit(X, y, epochs=50, verbose=1)\n",
    "\n",
    "###############################################################\n",
    "# توقع الكلمة التالية\n",
    "###############################################################\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_length):\n",
    "    # تحويل النص إلى تسلسل عددية\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = sequence[-max_sequence_length:]  # أخذ آخر خطوات زمنية فقط\n",
    "    sequence = np.array(sequence).reshape(1, -1)\n",
    "    \n",
    "    # التنبؤ بالكلمة التالية\n",
    "    predicted_probs = model.predict(sequence, verbose=0)[0]\n",
    "    predicted_word_index = np.argmax(predicted_probs)\n",
    "    \n",
    "    # تحويل التوقع إلى كلمة\n",
    "    word_index = tokenizer.word_index\n",
    "    index_word = {index: word for word, index in word_index.items()}\n",
    "    \n",
    "    return index_word.get(predicted_word_index, 'Unknown')\n",
    "\n",
    "# اختبار التنبؤ\n",
    "input_text = 'I love'\n",
    "predicted_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)\n",
    "print(f'Predicted next word: {predicted_word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformers With NLP**\n",
    "\n",
    "### ما هي المحولات (Transformers)؟\n",
    "\n",
    "المحولات هي بنية معمارية تعتمد بشكل أساسي على آلية الانتباه الذاتي (Self-Attention)، وهي تعمل على معالجة البيانات بشكل متوازي بدلًا من الاعتماد على التسلسل كما في الشبكات التكرارية. يتم ذلك من خلال تقسيم البيانات إلى وحدات صغيرة (تسمى **tokens**) ومعالجة كل وحدة على حدة مع الاحتفاظ بالعلاقات بين تلك الوحدات عبر الانتباه الذاتي.\n",
    "\n",
    "### المكونات الرئيسية للمحولات:\n",
    "\n",
    "1. **آلية الانتباه الذاتي (Self-Attention Mechanism)**:\n",
    "\n",
    "2. **التشفير (Encoder) وفك التشفير (Decoder)**:\n",
    "   - يتألف نموذج المحولات الأساسي من جزأين رئيسيين: **المشفر (Encoder)** و **فك التشفير (Decoder)**.\n",
    "     - **المشفر (Encoder)**: يتكون من عدة طبقات مسؤولة عن قراءة المدخلات وتحويلها إلى تمثيلات داخلية غنية بالمعلومات.\n",
    "     - **فك التشفير (Decoder)**: مسؤول عن توليد المخرجات، سواء كان ذلك نصًا مترجمًا أو إجابة على سؤال معين. يستخدم معلومات السياق التي تم معالجتها في المشفر.\n",
    "   \n",
    "3. **الطبقات متعددة الرؤوس للانتباه (Multi-Head Attention)**:\n",
    "   \n",
    "4. **التغذية الأمامية (Feed-Forward Layers)**:\n",
    "\n",
    "5. **التطبيع (Normalization) والاسقاط (Dropout)**:\n",
    "\n",
    "### آلية عمل المحولات:\n",
    "\n",
    "لشرح الآلية بتفصيل، دعنا نأخذ مثالًا بسيطًا لترجمة جملة من الإنجليزية إلى الفرنسية:\n",
    "\n",
    "1. **المدخلات**: يتم تقسيم الجملة المدخلة إلى وحدات صغيرة (tokens)، حيث كل كلمة أو جزء من الكلمة يتم تمثيله كـ \"token\".\n",
    "   \n",
    "2. **المشفر (Encoder)**: يقوم المشفر بمعالجة هذه الوحدات باستخدام آلية الانتباه الذاتي. هذا يسمح للنموذج بفهم العلاقات بين الكلمات داخل الجملة الإنجليزية.\n",
    "\n",
    "3. **التضمين (Embedding)**: كل كلمة يتم تحويلها إلى تمثيل رقمي عن طريق عملية التضمين (embedding).\n",
    "\n",
    "4. **الانتباه الذاتي**: كل وحدة من الجملة المدخلة تقيم علاقتها بباقي الوحدات الأخرى باستخدام آلية الانتباه الذاتي.\n",
    "\n",
    "5. **فك التشفير (Decoder)**: يعتمد فك التشفير على المعلومات التي تم توليدها من المشفر ليقوم بتوليد الترجمة بالفرنسية كلمةً كلمة.\n",
    "\n",
    "6. **النتيجة**: بعد المرور عبر عدة طبقات من الانتباه والتغذية الأمامية، ينتج النموذج الجملة المترجمة.\n",
    "\n",
    "### تطبيقات (Transformers) => (NLP):\n",
    "\n",
    "1. **الترجمة الآلية**:   \n",
    "2. **توليد النصوص (Text Generation)**:\n",
    "3. **الإجابة على الأسئلة (Question Answering)**:\n",
    "4. **تحليل المشاعر (Sentiment Analysis)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Self-Attention Mechanism (آلية الانتباه الذاتي)**\n",
    "\n",
    "آلية الانتباه الذاتي تمكن النموذج من تقييم العلاقات بين كل كلمة في الجملة والكلمات الأخرى. على سبيل المثال، في جملة مثل \"The cat sat on the mat\"، الانتباه الذاتي يسمح للنموذج بفهم العلاقة بين \"cat\" و \"mat\" لأنهما يشتركان في سياق الجملة.\n",
    "\n",
    "#### كيف تعمل؟\n",
    "\n",
    "1. لكل كلمة، يتم إنشاء تمثيل ثلاثي:\n",
    "   - **Query**: السؤال أو الاستفسار عن العلاقات.\n",
    "   - **Key**: ما إذا كانت هذه الكلمة مهمة لكلمات أخرى.\n",
    "   - **Value**: المعلومات المرتبطة بالكلمة.\n",
    "\n",
    "2. يتم حساب درجة الانتباه لكل كلمة من خلال مقارنة الـ Query مع باقي الـ Keys. \n",
    "\n",
    "3. يتم ضرب كل قيمة Value في الأهمية المستنتجة من خطوة الانتباه.\n",
    "\n",
    "#### معادلة الانتباه الذاتي:\n",
    "#### dimension = 512 <!-- عدد الابعاد--> \n",
    "#### sequence_num = 4  <!-- عدد العناصر--> \n",
    "#### Q= 4*512\n",
    "#### K^T= 512*4 \n",
    "#### attention_weights = softmax(query * k^T / sqrt(dimension))\n",
    "\n",
    "### 2. **التشفير (Encoder) وفك التشفير (Decoder)**\n",
    "\n",
    "#### المشفر (Encoder):\n",
    "المشفر يتكون من عدة طبقات من الانتباه الذاتي والتغذية الأمامية. يقوم المشفر بتحليل وفهم النص المدخل (مثل الجملة) عبر مراحل متعددة.\n",
    "\n",
    "#### فك التشفير (Decoder):\n",
    "فك التشفير يستخدم تمثيلات النص التي تم توليدها من المشفر، ويعتمد على هذه التمثيلات لتوليد المخرجات، مثل ترجمة الجملة إلى لغة أخرى أو توليد نص جديد.\n",
    "\n",
    "### 3. **Multi-Head Attention (الانتباه متعدد الرؤوس)**\n",
    "\n",
    "الانتباه متعدد الرؤوس هو نسخة محسنة من الانتباه الذاتي، حيث يتم تقسيم التمثيلات إلى \"رؤوس\" متعددة. كل \"رأس\" يقوم بحساب الانتباه بشكل مختلف، مما يسمح للنموذج بالنظر إلى جوانب متعددة من العلاقات بين الكلمات في نفس الوقت.\n",
    "\n",
    "#### مثال:\n",
    "- يمكن لرأس واحد أن يركز على العلاقة بين الفاعل والفعل، بينما رأس آخر يركز على العلاقة بين الفعل والمفعول.\n",
    "\n",
    "### 4. **Feed-Forward Layers (طبقات التغذية الأمامية)**\n",
    "\n",
    "بعد حساب الانتباه، يتم تمرير النتائج عبر شبكة عصبية مكونة من عدة طبقات تغذية أمامية. هذه الطبقات تكون مسؤولة عن تحسين التمثيلات بعد حساب الانتباه.\n",
    "\n",
    "### 5. **Normalization (التطبيع) و Dropout (الاسقاط)**\n",
    "\n",
    "- **التطبيع (Layer Normalization)**: يساعد في تسريع عملية التدريب عبر ضمان استقرار التدفقات الداخلية للمعلومات.\n",
    "- **الإسقاط (Dropout)**: تقنية لتجنب الإفراط في التعلم عبر إسقاط بعض الوحدات العشوائية من الشبكة العصبية أثناء التدريب."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " [[[0.37360093 0.17979056 0.03352834 0.86513391 0.37873786 0.64912537\n",
      "   0.99745314 0.37249064]\n",
      "  [0.04876278 0.69262128 0.06985811 0.02680683 0.17189162 0.65845445\n",
      "   0.65553791 0.4316282 ]\n",
      "  [0.31856327 0.27139831 0.72847708 0.27890023 0.88647585 0.04556948\n",
      "   0.71748802 0.46323336]\n",
      "  [0.70252251 0.83871822 0.35630011 0.17419226 0.681343   0.28245652\n",
      "   0.13958494 0.23563952]\n",
      "  [0.69079128 0.23013696 0.65516551 0.16379091 0.51421659 0.01989726\n",
      "   0.71982381 0.31255854]]]\n",
      "\n",
      " Multi-Head Attention:\n",
      " tf.Tensor(\n",
      "[[[ 0.3694607  -0.11383264  0.07093222  0.07887593  0.08018263\n",
      "   -0.07615816 -0.22729266  0.01020066]\n",
      "  [ 0.37022325 -0.11489406  0.07142335  0.07987761  0.07943211\n",
      "   -0.07571764 -0.2280041   0.01010716]\n",
      "  [ 0.36997628 -0.11530428  0.07105333  0.08184449  0.07616985\n",
      "   -0.07275891 -0.23105794  0.01009158]\n",
      "  [ 0.36925873 -0.11561364  0.07059392  0.08129579  0.07673295\n",
      "   -0.07296948 -0.23033693  0.01080636]\n",
      "  [ 0.36844093 -0.1141924   0.0702979   0.08042189  0.07750918\n",
      "   -0.07407347 -0.2294211   0.01039292]]], shape=(1, 5, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# طول التسلسل و حجم التمثيل\n",
    "sequence_length = 5  # عدد الكلمات في الجملة\n",
    "embedding_dim = 8    # حجم التمثيل لكل كلمة\n",
    "num_heads = 2        # عدد الرؤوس في Multi-Head Attention\n",
    "\n",
    "# مثال بسيط على المدخلات (تمثيلات عشوائية لكل كلمة)\n",
    "# نفترض أن لدينا 5 كلمات وكل كلمة ممثلة بتمثيل حجمه 8\n",
    "dummy_input = np.random.rand(1, sequence_length, embedding_dim)\n",
    "\n",
    "# 1. تعريف طبقة Multi-Head Attention من Keras\n",
    "multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "\n",
    "# 2. تطبيق Multi-Head Attention على المدخلات\n",
    "# في آلية Attention، Q = K = V عندما نتعامل مع Self-Attention\n",
    "attention_output = multi_head_attention(query=dummy_input, value=dummy_input, key=dummy_input)\n",
    "\n",
    "# 3. بناء نموذج Keras بسيط لعرض النتائج\n",
    "input_layer = layers.Input(shape=(sequence_length, embedding_dim))\n",
    "attention_layer = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(input_layer, input_layer)\n",
    "model = Model(inputs=input_layer, outputs=attention_layer)\n",
    "\n",
    "# 4. طباعة النتائج\n",
    "print(\"input:\\n\", dummy_input)\n",
    "print(\"\\n Multi-Head Attention:\\n\", attention_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
